Index: Stock_Data_Analysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from Stock_Market_Application import *\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Create a dictionary to store all the stock dataframes and its name\r\nstock_data = {'MSFT':data_msft,'GOOG': data_goog,'AMZN': data_amzn,'AAPL': data_aapl,'SAP':data_sap,'META':data_meta,'005930_KS':data_005930_ks,'INTC':data_intc,\r\n              'IBM':data_ibm, 'ORCL':data_orcl, 'BABA':data_baba,'TCEHY':data_tcehy,'NVDA': data_nvda,'TSM':data_tsm,'NFLX': data_nflx,'TSLA':data_tsla,'CRM':data_crm,\r\n              'ADBE':data_adbe,'PYPL':data_pypl}\r\n'''a dictionary can store a dataframe'''\r\n\r\n# A function to analyse data structures of stock data\r\ndef analyse_stock_data(stock_data):\r\n    for i, (name,data) in enumerate(stock_data.items()):\r\n        '''taking 2 lists of stock data and stock names and returning an iterator aggregating\r\n        elements from each list. Also print the info, shape (numbers of columns and rows), data type and the table of statistical descriptions.\r\n        of each stock.  \r\n        Inputs:\r\n        stock_data (dataframes): list of stock dataframes\r\n        stock_names (string): list of stock names\r\n        Returns:\r\n        string: stock name\r\n        string: information about the stock\r\n        string: numbers of columns and rows\r\n        string: data types of each column in the stock dataframe\r\n        string: stats of the stock dataframe \r\n        '''\r\n        print(f'{name}')\r\n        print(\"\\nInfo:\")\r\n        print(data.info())\r\n        print(\"\\nShape:\")\r\n        print(data.shape)\r\n        print(\"\\nData Types:\")\r\n        print(data.dtypes)\r\n        print(\"\\nDescribe:\")\r\n        print(data.describe())\r\n        print(data.duplicated().sum())\r\n        print(\"\\n Number of duplicated values: \" + str(data.duplicated().sum()))\r\n\r\n\r\nanalyse_stock_data(stock_data)\r\n#Convert the DataFrame to an Agate table\r\n#table = ag.Table.from_object(data_msft)\r\n#print(table.column_names)\r\n\r\n#new_table = table.select(['column_name'])\r\n\r\n#Checking outliers in our stock data using the method of Interquartile Range(IQR).\r\n#The IQR is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data.\r\n#Any point falling below Q1 - 1.5IQR or above Q3 + 1.5IQR is considered an outlier.\r\n\r\nfor i,(name,data) in enumerate(stock_data.items()):\r\n    Q1 = data['Close'].quantile(0.25)\r\n    Q3 = data['Close'].quantile(0.75)\r\n    IQR = Q3 - Q1\r\n    #Define bounds for outliers\r\n    lower_bound = Q1 - 1.5 * IQR\r\n    upper_bound = Q3 + 1.5 * IQR\r\n    #Detect outliers\r\n    data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]\r\n    if data_outliers.empty:\r\n        print(f\"The outliers of {name} is none\")\r\n    else:\r\n        print(f\"The outliers of {name} is {data_outliers['Close']}\")\r\n        print(f\"Number of outliers: {data_outliers.shape[0]}\")\r\n    #Create boxplot\r\n    plt.boxplot(data['Close'])\r\n    plt.title (f\"Box Plot of {name}\")\r\n    plt.show()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Stock_Data_Analysis.py b/Stock_Data_Analysis.py
--- a/Stock_Data_Analysis.py	(revision c196cf49e467f6b5f5ce0aa5db7f273c1715d3b7)
+++ b/Stock_Data_Analysis.py	(date 1710222994557)
@@ -1,8 +1,17 @@
 from Stock_Market_Application import *
 import matplotlib.pyplot as plt
 import pandas as pd
-
+import seaborn as sns
+import numpy as np
+import pylab
+import scipy.stats as stats
+from statsmodels.tsa.seasonal import seasonal_decompose
+from datetime import datetime
+import plotly.graph_objects as go
+from sklearn.preprocessing import StandardScaler
+from sklearn.decomposition import PCA
 #Create a dictionary to store all the stock dataframes and its name
+
 stock_data = {'MSFT':data_msft,'GOOG': data_goog,'AMZN': data_amzn,'AAPL': data_aapl,'SAP':data_sap,'META':data_meta,'005930_KS':data_005930_ks,'INTC':data_intc,
               'IBM':data_ibm, 'ORCL':data_orcl, 'BABA':data_baba,'TCEHY':data_tcehy,'NVDA': data_nvda,'TSM':data_tsm,'NFLX': data_nflx,'TSLA':data_tsla,'CRM':data_crm,
               'ADBE':data_adbe,'PYPL':data_pypl}
@@ -11,12 +20,11 @@
 # A function to analyse data structures of stock data
 def analyse_stock_data(stock_data):
     for i, (name,data) in enumerate(stock_data.items()):
-        '''taking 2 lists of stock data and stock names and returning an iterator aggregating
+        '''Takes a dictionary of stock data and returning an iterator aggregating
         elements from each list. Also print the info, shape (numbers of columns and rows), data type and the table of statistical descriptions.
         of each stock.  
         Inputs:
-        stock_data (dataframes): list of stock dataframes
-        stock_names (string): list of stock names
+        stock_data (dictionary): A dictionary of stock data and their corresponding names
         Returns:
         string: stock name
         string: information about the stock
@@ -35,20 +43,216 @@
         print(data.describe())
         print(data.duplicated().sum())
         print("\n Number of duplicated values: " + str(data.duplicated().sum()))
-
-
 analyse_stock_data(stock_data)
-#Convert the DataFrame to an Agate table
-#table = ag.Table.from_object(data_msft)
-#print(table.column_names)
+
+#A dictionary to store all the stock data with reset DateTimeindex indices
+stock_reset_index = {}
+#A function to reset DateTimeIndex for each dataframe
+def reset_index_stock_data(stock_data):
+    '''Takes a dictionary of stock data and resets the index of time.
+    Input:
+    stock_data(dictionary): a dictionary containing the stock data and its name
+    Returns:
+    stock_reset_index(dictionary): Dictionary with stock names as keys and stock data as values with reset Date indices
+    '''
+    for i,(name,data) in enumerate(stock_data.items()):
+        data_reset = data.reset_index()
+        stock_reset_index[name + '_reset'] = data_reset
+    print(stock_reset_index)
+
+reset_index_stock_data(stock_data)
+
+#Pair plots for stock market data
+for i,(name,data) in enumerate(stock_reset_index.items()):
+    plt.figure()
+    sns.pairplot(data)
+    plt.title(name)
+    plt.show()
+plt.close('all')
+
+#Principal Component Analysis for stock market data to identify dominant patterns and understand relationships
+def principal_analysis(df):
+    '''
+
+     Takes a dictionary of stock data and returns values of explained variance ratios for
+     principal components, scatter plots of principal components and dataframes of loadings
+     Input:
+     stock_data(dictionary): a dictionary containing the stock data and their names
+
+     Returns:
+     string: explained variance ratios for principal components of stock data
+     dataframe: loadings of principal components of stock data
+     graph: scatter plots of principal components of stock data
+            '''
+    for i,(name,data) in enumerate(stock_data.items()):
+        scaler = StandardScaler() #scaling our data with standard scaler
+        scaled_data = scaler.fit_transform(data)
+
+        n_components = 6 #specifying the number of dimensions we want to keep
+        pca = PCA(n_components=n_components)
+        principal_components = pca.fit_transform(scaled_data)
+        #Convert to DataFrame
+        component_names = [f"PC{j+1}" for j in range(principal_components.shape[1])]
+        principal_df = pd.DataFrame(principal_components, columns=component_names)
+        principal_df.head()
+
+        #Check how much variance each principal component explains
+        explained_variance = pca.explained_variance_ratio_
+        print(f"Explained variance ratio for {name}: {explained_variance}")
+
+        #Calculate the correlations/covariance between the original features and PCA-scaled units
+        loadings = pd.DataFrame(
+            pca.components_.T, #transpose the matrix of loadings
+            columns = component_names,# the columns a re the principal components
+            index = data.columns, #the rows are the original features
+        )
+        print(loadings)
+        # Visualise the reduced-dimensional data
+
+        plt.figure(figsize=(8, 6))
+        sns.scatterplot(data=principal_components)  # creating a scatter plot for the principal components of all the data points for a company
+        plt.title(f"Principal Components for {name}")
+        plt.show()
+        plt.close('all')
+
+        def plot_variance(pca):
+            '''Takes PCA components and their corresponding explained variance
+             and returns their plots of explained variance.
+            Input:
+            pca components: a string of pca components of all features in stock market data
+            explained_variance: a list of explained variance for each pca component
+            Returns:
+            plot: a list of plots of explained variance for each pca
+            '''
+            fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 8))
+            n = pca.n_components
+            grid = np.arange(1, n + 1)
+
+            # Explained variance
+            explained_variance = pca.explained_variance_ratio_
+            axs[0].bar(grid, explained_variance)
+            axs[0].set(
+                xlabel="Component", ylabel="% Explained Variance",
+                ylim=(0.0, 1.0)
+
+            )
+            # Cumulative Variance
+            cv = np.cumsum(explained_variance)
+            axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")
+            axs[1].set(
+                xlabel="Component", title="% Cumulative Variance",
+                ylim=(0.0, 1.0)
+            )
+            # Set up figure
+            plt.title(f"Variance for {name}")
+            fig.set(figwidth=8, dpi=100)
+            plt.close('all')
+            return axs
+
+        plot_variance(pca)
+principal_analysis(stock_reset_index)
+
+
+#Probability Plot to determine whether our data follow a specific distribution.
+cols = ['Open', 'High', 'Low', 'Adj Close', 'Close', 'Volume']
+def detect_normal_distribution(stock_data):
+    for i, (name,data) in enumerate(stock_data.items()):
+        fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 8))
+        axs = axs.flatten() #flatten (meaning to transform into a one-dimensional array)
+        # the axes array to easily iterate over it
+        for ax, col in zip(axs,cols): #using zip function to iterate over the axes and column names
+            #simultaneously
+            stats.probplot(data[col], dist='norm', plot=ax) #Q-Q Plot
+            ax.set_title(f"Probability Plot of {col} for {name}")
+        plt.tight_layout()
+        plt.show()
+#Create a dictionary to store stock names and their dataframes
+detect_normal_distribution(stock_data)
 
-#new_table = table.select(['column_name'])
 
-#Checking outliers in our stock data using the method of Interquartile Range(IQR).
+
+#Line Plot for stock data
+trace_names = ['Open', 'High', 'Low', 'Adj Close', 'Close']
+for i,(name,reset_data) in enumerate(stock_reset_index.items()):
+    fig = go.Figure()
+    for trace_name in trace_names:
+        fig.add_trace(go.Scatter(x=reset_data['Date'], y=reset_data[trace_name],mode='lines',name='Open'))
+    #fig.add_trace(go.Scatter(x=reset_data['Date'], y=reset_data['Volume'], mode='lines', name='Volume'))
+    fig.update_layout(title=f"Stock Price for {name}")
+    fig.show()
+
+#Statistical Analysis to understand the data distribution
+#def detect_normal_distribution(stock_data):
+ #   '''Takes a dictionary of stock_data and returning histograms, shapiro test results and kolmogorov-smirnov test results
+  #  Input:
+   # stock_data (dict): Dictionary of stock data and their names
+
+#    Returns:
+ #   graph: Histogram of stock data and their names
+  #  string: Shapiro test results of stock data
+   # string: Kolmogorov-Smirnov test results of stock data
+   # '''
+
+    #for i, (name,data) in enumerate(stock_data.items()):
+     #   fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 8))
+      #  axs = axs.flatten()  # flatten (meaning to transform into a one-dimensional array)
+       # for (ax,col) in zip(axs,cols):
+        #  decomposition = seasonal_decompose(data[col],model= 'multiplicative', period=7)
+
+         # trend = decomposition.trend
+         # seasonal = decomposition.seasonal
+         # residual = decomposition.resid
+          #Plot the original data, trend, seasonality and residuals
+         # plt.subplot(411)
+         # plt.plot(data[col],label='Original')
+          #plt.legend(loc='best')
+         # plt.subplot(412)
+         # plt.plot(trend,label='Trend')
+         # plt.legend(loc='best')
+         # plt.subplot(413)
+         # plt.plot(seasonal,label='Seasonality')
+         # plt.legend(loc='best')
+         # plt.subplot(414)
+         # plt.plot(residual,label='Residuals')
+         # plt.legend(loc='best')
+         # plt.tight_layout()
+    #plt.close('all')
+          #shapiro_test = stats.shapiro(data[col]) #Shapiro-Wilk Test
+          #print(f"{name}'s Shapiro Test for {col}: {shapiro_test[0]}, p-value:{shapiro_test[1]}")
+          #if shapiro_test[1] < 0.05:
+          #  print(f"Based on Shapiro test, {name}'s {col} may not be a normal distribution.")
+          #else:
+          #  print (f"Based on Shapiro test, there is not enough evidence to suggest that {name}'s {col} may not be a normal distribution.")
+
+           # kolmo_test = stats.kstest(data[col], 'norm') #Kolmogorov-Smirnov test
+          #  print (f"{name}'s KS test for {col}: {kolmo_test.statistic}, p-value:{kolmo_test.pvalue}")
+          #  if kolmo_test.pvalue < 0.05:
+           #     print (f"Based on KS test, {name}'s {col} may not be a normal distribution.")
+           # else:
+            #    print(f"Based on KS test, there is not enough statistic evidence to suggest that {name}'s {col} may not be a normal distribution.")
+#detect_normal_distribution(stock_data)
+
+
+
+
+#Investigating outliers in our stock data using the method of Interquartile Range(IQR) since all stock data are not normally
+# distributed.
+# It is a common technique used for stock market because it gives us insights into the spread of stock prices
+# over a specific period. It is resistant to outliers, making it robust.
+#An outlier: an extremely high or low data point relative to the nearest data points and
+#the rest of the neighboring co-existing vals in a dataset.
 #The IQR is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data.
 #Any point falling below Q1 - 1.5IQR or above Q3 + 1.5IQR is considered an outlier.
 
 for i,(name,data) in enumerate(stock_data.items()):
+    '''Takes stock data and returns and box plots outliers of them.
+    Input:
+    stock_data: a dictionary of stock data
+    Returns:
+    plot: a box plot of the stock data
+    Series: a series of outliers for each stock
+    '''
+
     Q1 = data['Close'].quantile(0.25)
     Q3 = data['Close'].quantile(0.75)
     IQR = Q3 - Q1
@@ -58,14 +262,26 @@
     #Detect outliers
     data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]
     if data_outliers.empty:
-        print(f"The outliers of {name} is none")
+        print(f"The outliers of {name} are none")
     else:
-        print(f"The outliers of {name} is {data_outliers['Close']}")
+        print(f"The outliers of {name} are {data_outliers['Close']}")
         print(f"Number of outliers: {data_outliers.shape[0]}")
     #Create boxplot
     plt.boxplot(data['Close'])
     plt.title (f"Box Plot of {name}")
     plt.show()
+    plt.close('all')
+
+ #Heat maps to show the correlation between different features
+ for i,(name,data) in enumerate(stock_data.items()):
+     #Calculate the correlation matrix
+     feat_corr = data.corr()
+
+     #Create the heat map of the correlation matrix
+     plt.figure(figsize=(10,8))
+     sns.heatmap(corr, cmap='coolwarm',annot=True)
+     plt.title (f" The correlation matrix for {name}")
+     plt.show()
 
 
 
@@ -82,3 +298,9 @@
 
 
 
+
+
+
+
+
+
Index: Data Structures of Stock Data.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Data Structures of Stock Data.py b/Data Structures of Stock Data.py
new file mode 100644
--- /dev/null	(date 1704342325415)
+++ b/Data Structures of Stock Data.py	(date 1704342325415)
@@ -0,0 +1,1 @@
+data_msft
\ No newline at end of file
Index: .idea/ASX.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\">\r\n      <excludeFolder url=\"file://$MODULE_DIR$/.venv\" />\r\n    </content>\r\n    <orderEntry type=\"inheritedJdk\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/ASX.iml b/.idea/ASX.iml
--- a/.idea/ASX.iml	(revision c196cf49e467f6b5f5ce0aa5db7f273c1715d3b7)
+++ b/.idea/ASX.iml	(date 1709184795213)
@@ -1,9 +1,7 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
-    <content url="file://$MODULE_DIR$">
-      <excludeFolder url="file://$MODULE_DIR$/.venv" />
-    </content>
+    <content url="file://$MODULE_DIR$" />
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
Index: Stock Analysis.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Stock Analysis.md b/Stock Analysis.md
new file mode 100644
--- /dev/null	(date 1710217101603)
+++ b/Stock Analysis.md	(date 1710217101603)
@@ -0,0 +1,115 @@
+***Missing values***
+- MSFT: no missing values
+- AMZN: no missing values
+- APPL: no missing values
+- SAP: no missing values
+- META: no missing values
+- OO5930_KS (Samsung): no missing values
+- INTC: no missing values
+- IBM: no missing values
+- ORCL: no missing values
+- BABA: no missing values
+- TCEHY:no missing values
+- NVDA: no missing values
+- TSM ( Taiwan Semiconductor Manufacturing Company Limited): no missing values
+- NFLX: no missing values
+- TSLA: no missing values
+- CRM: no missing values
+- ADBE: no missing values
+- PYPL: no missing values
+*** Duplicated values ***
+- MSFT: none
+- GOOG: none
+- AMZN: none
+- APPL: none
+- SAP: none
+- META: none
+- 005930_KS: none
+- INTC: none
+- IBM: none
+- ORCL: none
+- BABA: none
+- TCEHY: none
+- NVDA: none
+- TSM: none
+- NFLX: none
+- TSLA: none
+- CRM: none
+- ADBE: none
+- PYPL: none
+
+***Outliers***
+- The formula of detecting outliers using the IQR depends on statistic dispersion and the characteristics of a box plot. 
+We know lower and upper bounds as the "outlier gates"
+- It is based on the properties of the normal distribution and the desire to classify as outliers any extreme data points, typically far from the median.
+a) Q3 situates at approx 0.675 SD from the median for a normal distribution.
+b) The IQR (Q3 - Q1) represents approx 1.35 SD.
+c) Adding 1.5 * IQR to Q3 ( or subtracting it from Q1) is therefore approx equivalent to defining outliers as points
+that are more than 2.7 SD from the median. 
+d) In a normal distribution, approx 0.7% of data falls more than 2.7 SD from the mean. So, this will classify only the most
+extreme 0.7% of the data as outliers. 
+e) In different fields, we may use different multipliers instead of 1.5, depending on how inclusive or exclusive they want
+the outlier definition to be. 
+- Outliers:
+MSFT: none
+GOOG: none
+AMZN: none 
+AAPL: none
+SAP: none
+META: 16
+005930_KS: none
+IBM: 82
+ORCL: 49
+BABA: none
+TCEHY: 15
+NVDA: 143
+TSM: none
+NFLX: none
+TSLA: none
+CRM: none
+ADBE: none
+PYPL: 270
+Outliers in the 'Close' prices may occur for various reasons: market volatility, significant news or events, earning reports (investors
+use earnings reports to gauge a company's future profitability and adjust their investment decisions accordingly earnings per share is a common
+ratio investors use to assess a company's profitability, P/E (price to earnings) ratio => calculated by dividing the share price of a company
+by its EPS, a high P/E ration compared to others in the same industry may indicate that the company is overvalued, while a low P/E could indidate that
+the company is undervalued, earnings yield is an inverse of the P/E ratio), changes in market sentiments, data entry errors.
+Highly unlikely in this case that there will be data entry errors. 
+Researchers often view outliers as 'data problems', and they tend to overlook the fact that outliers can be substantively interesting
+and studied as unique phenomena that could lead to novel theoretical insights. There is, therefore, a need for a better understanding
+and clear guidelines regarding the following 3 issues: a) How to define them. b) how to identify them, c) how to handle them. 
+How to best handle outliers:
+https://journals.sagepub.com/stoken/default+domain/10.1177/1094428112470848/full
+
+https://www.tandfonline.com/doi/pdf/10.1080/23322039.2022.2066762
+
+- Meta: outliers from the period of 2021-07-26 till 2021-09-09 =>  The Dow Jones Industrial Average also experienced some changes during this period
+(could be due to economic indicators, geopolitical events, and changes in investor sentiment) => maybe Covid-19, more people relying on online communication?
+Amid ongoing concerns about its struggles to adequately protect data and limit hate speech, 
+misinformation and other disreputable content, the world’s largest social network confronted a flood of issues this year,
+beginning with the Capitol insurrection and its subsequent decision to indefinitely suspend then-President Trump. 
+Troves of documents later leaked by former Facebook employee turned whistleblower Frances Haugen revealed more damaging information about the impact of the company’s platforms on young users’ mental health. 
+Finally, Facebook announced that it was rebranding itself 
+as Meta to reflect a focus on the metaverse, a virtual reality space where users interact with each other amid a computer-generated environment.
+But the outliers occurred before its rebranding. 
+Why outliers during this period? => natural variation or something else? (need more investigation)
+- IBM: outliers occurred from 2014-01-07 till 2020-03-23. 2014-2017: Bull Market Continues: The stock market, as represented by indices like the Dow Jones Industrial Average (DJIA) and the S&P/ASX 200
+generally trended upwards during this period. => corresponding to IBM's upper bound outliers.
+2018: Increased Volatility: The year 2018 saw increased market volatility, with the DJIA and other indices experiencing several notable dips. 
+This was due in part to concerns about global trade tensions and changes in monetary policy
+2019: Market Recovery: Despite the volatility in 2018, the market bounced back in 2019, with the DJIA and other indices reaching new all-time highs
+2020: COVID-19 Impact: The global outbreak of the COVID-19 pandemic in early 2020 had a significant impact on the stock market. 
+In March 2020, the DJIA and other global indices experienced dramatic drops, marking the end of the bull market that had begun in 2009 => corresponding to the lower bound outliers from 2020-03-16 till 2020-03-23 
+
+
+***Correlation***
+- For all the stocks, Open, High, Low, Close and Adj Close are all correlated to each other (1). 
+There are no correlations between Open, High, Low, Close, Adj Close with Volume. 
+
+***Q-Q Plots***
+- None of the stock data follow a normal distribution
+
+*** Linear relationships***
+- Seaborn pair plots show that in all companies, "Open", "High","Low",Close" and "Adj Close" all show 
+linear relationships with each other. No strong relationships between "Volume" and the rest of other features exhibited (need 
+to calculate correlation coefficients to make sure)
\ No newline at end of file
Index: Building a Dash app.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Building a Dash app.md b/Building a Dash app.md
new file mode 100644
--- /dev/null	(date 1707103561870)
+++ b/Building a Dash app.md	(date 1707103561870)
@@ -0,0 +1,118 @@
+- A Dash app is usually composed of 2 parts. The 1st one is the "layout" of the app and describes what the app looks like. 
+The 2nd part describe the interactivity of the app
+- Code dash.Dash(__name__): create a new Dash application:
+1. dash.Dash: constructor (in class-based, object-oriented programming, a special type of function called to create an
+object, preparing the new object for use, often accepting arguments that the constructor uses to set required member variables) for the app
+2. __name__: a special variable in Python, set to the name of the module where it is used 
+=> creating a new Dash app, using the name of the current module as the name of the Flask server Dash uses under the hood. 
+- @app.callback: decorator used to create callback functions to make your Dash apps interactive. 
+- plotly.graph_objects: providing a low-level interface to the underlying data structures defining the charts, the low-level interface of Plotly, 
+providing more flexibility and control over the details of the plot.
+Containing the building blocks of Plotly figures: traces for i,(name,data) in enumerate(stock_data.items()):
+    Q1 = data['Close'].quantile(0.25)
+    Q3 = data['Close'].quantile(0.75)
+    IQR = Q3 - Q1
+    #Define bounds for outliers
+    lower_bound = Q1 - 1.5 * IQR
+    upper_bound = Q3 + 1.5 * IQR
+    #Detect outliers
+    data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]
+    lower_bound_outliers = data[(data['Close'] < lower_bound)]
+    upper_bound_outliers = data[(data['Close'] > upper_bound)]
+    if data_outliers.empty:
+        print(f'The outliers of {name} is none')
+    if lower_bound_outliers.empty:
+        print(f'The lower bound outliers for {name} is none')
+    if upper_bound_outliers.empty:for i,(name,data) in enumerate(stock_data.items()):
+    Q1 = data['Close'].quantile(0.25)
+    Q3 = data['Close'].quantile(0.75)
+    IQR = Q3 - Q1
+    #Define bounds for outliers
+    lower_bound = Q1 - 1.5 * IQR
+    upper_bound = Q3 + 1.5 * IQR
+    #Detect outliers
+    data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]
+    lower_bound_outliers = data[(data['Close'] < lower_bound)]
+    upper_bound_outliers = data[(data['Close'] > upper_bound)]
+    if data_outliers.empty:
+        print(f'The outliers of {name} is none')
+    if lower_bound_outliers.empty:
+        print(f'The lower bound outliers for {name} is none')
+    if upper_bound_outliers.empty:
+        print(f'The upper bound outliers for {name} is none')
+    else:
+        print(f'The outliers of {name} is {data_outliers['Close']}')
+        print(f'Number of outliers: {data_outliers.shape[0]}')
+        print(f'The lower bound outliers for {name} is {lower_bound_outliers['Close']}')
+        print(f'The upper bound outliers for {name} is {upper_bound_outliers['Close']}')
+    #Create boxplot
+    plt.boxplot(data['Close'])
+    plt.title (f'Box Plot of {name}')
+    plt.show()for i,(name,data) in enumerate(stock_data.items()):
+    Q1 = data['Close'].quantile(0.25)
+    Q3 = data['Close'].quantile(0.75)
+    IQR = Q3 - Q1
+    #Define bounds for outliers
+    lower_bound = Q1 - 1.5 * IQR
+    upper_bound = Q3 + 1.5 * IQR
+    #Detect outliers
+    data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]
+    lower_bound_outliers = data[(data['Close'] < lower_bound)]
+    upper_bound_outliers = data[(data['Close'] > upper_bound)]
+    if data_outliers.empty:
+        print(f'The outliers of {name} is none')
+    if lower_bound_outliers.empty:
+        print(f'The lower bound outliers for {name} is none')
+    if upper_bound_outliers.empty:
+        print(f'The upper bound outliers for {name} is none')
+    else:
+        print(f'The outliers of {name} is {data_outliers['Close']}')
+        print(f'Number of outliers: {data_outliers.shape[0]}')
+        print(f'The lower bound outliers for {name} is {lower_bound_outliers['Close']}')
+        print(f'The upper bound outliers for {name} is {upper_bound_outliers['Close']}')
+    #Create boxplot
+    plt.boxplot(data['Close'])
+    plt.title (f'Box Plot of {name}')
+    plt.show()for i,(name,data) in enumerate(stock_data.items()):
+    Q1 = data['Close'].quantile(0.25)
+    Q3 = data['Close'].quantile(0.75)
+    IQR = Q3 - Q1
+    #Define bounds for outliers
+    lower_bound = Q1 - 1.5 * IQR
+    upper_bound = Q3 + 1.5 * IQR
+    #Detect outliers
+    data_outliers = data[(data['Close'] < lower_bound) | (data['Close'] > upper_bound)]
+    lower_bound_outliers = data[(data['Close'] < lower_bound)]
+    upper_bound_outliers = data[(data['Close'] > upper_bound)]
+    if data_outliers.empty:
+        print(f'The outliers of {name} is none')
+    if lower_bound_outliers.empty:
+        print(f'The lower bound outliers for {name} is none')
+    if upper_bound_outliers.empty:
+        print(f'The upper bound outliers for {name} is none')
+    else:
+        print(f'The outliers of {name} is {data_outliers['Close']}')
+        print(f'Number of outliers: {data_outliers.shape[0]}')
+        print(f'The lower bound outliers for {name} is {lower_bound_outliers['Close']}')
+        print(f'The upper bound outliers for {name} is {upper_bound_outliers['Close']}')
+    #Create boxplot
+    plt.boxplot(data['Close'])
+    plt.title (f'Box Plot of {name}')
+    plt.show()
+        print(f'The upper bound outliers for {name} is none')
+    else:
+        print(f'The outliers of {name} is {data_outliers['Close']}')
+        print(f'Number of outliers: {data_outliers.shape[0]}')
+        print(f'The lower bound outliers for {name} is {lower_bound_outliers['Close']}')
+        print(f'The upper bound outliers for {name} is {upper_bound_outliers['Close']}')
+    #Create boxplot
+    plt.boxplot(data['Close'])
+    plt.title (f'Box Plot of {name}')
+    plt.show()and Layout. Scenarios: customisation, complex charts, learning purpose 
+- dash_html_components depreciated => need to use 'from dash import html'
+- In Dash, 'my-id' and 'value' mean the id and value properties of a Dash component. 
+'my-id': unique identifier of a Dash component. 
+'value': holding the current value of the component
+- The 'inputs' and 'outputs' of the application are described as the arguments of the @callback
+decorator. In Dash, the inputs and outputs are simply the properties of a particular component.
+Whenever an input property changes, the function that the callback decorator wraps will get called automatically. 
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"4f5895f3-2d93-4355-bcbb-97c6701d0028\" name=\"Changes\" comment=\"\">\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/ASX.iml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/misc.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/modules.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/other.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/vcs.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Building a Dash app.md\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Data Structures of Stock Data.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Stock Analysis.md\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Stock_Data_Analysis.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Stock_Market_Application.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/main.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/stock_market_app_review.md\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"FlaskConsoleOptions\" custom-start-script=\"import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\\nApp: %s [%s]\\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))\">\r\n    <envs>\r\n      <env key=\"FLASK_APP\" value=\"app\" />\r\n    </envs>\r\n    <option name=\"myCustomStartScript\" value=\"import sys&#10;sys.path.extend([WORKING_DIR_AND_PYTHON_PATHS])&#10;from flask.cli import ScriptInfo&#10;locals().update(ScriptInfo(create_app=None).load_app().make_shell_context())&#10;print(&quot;Python %s on %s\\nApp: %s [%s]\\nInstance: %s&quot; % (sys.version, sys.platform, app.import_name, app.env, app.instance_path))\" />\r\n    <option name=\"myEnvs\">\r\n      <map>\r\n        <entry key=\"FLASK_APP\" value=\"app\" />\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProblemsViewState\">\r\n    <option name=\"selectedTabId\" value=\"CurrentFile\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 5\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2ZqF91uoUhatta9TbVHHxhZj3cs\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\"><![CDATA[{\r\n  \"keyToString\": {\r\n    \"Python.Stock_Data_Visualisation.executor\": \"Run\",\r\n    \"Python.Stock_Market_Application.executor\": \"Run\",\r\n    \"RunOnceActivity.OpenProjectViewOnStart\": \"true\",\r\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\r\n    \"SHARE_PROJECT_CONFIGURATION_FILES\": \"true\",\r\n    \"git-widget-placeholder\": \"master\",\r\n    \"last_opened_file_path\": \"C:/Users/laran/PycharmProjects/ASX\",\r\n    \"node.js.detected.package.eslint\": \"true\",\r\n    \"node.js.detected.package.tslint\": \"true\",\r\n    \"node.js.selected.package.eslint\": \"(autodetect)\",\r\n    \"node.js.selected.package.tslint\": \"(autodetect)\",\r\n    \"nodejs_package_manager_path\": \"npm\",\r\n    \"settings.editor.selected.configurable\": \"reference.settingsdialog.IDE.editor.colors.VCS\",\r\n    \"vue.rearranger.settings.migration\": \"true\"\r\n  }\r\n}]]></component>\r\n  <component name=\"RunManager\">\r\n    <configuration name=\"Stock_Market_Application\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"ASX\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/Stock_Market_Application.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.Stock_Market_Application\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-python-sdk-7a29c1521ef0-c986f194a52a-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-233.11799.298\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"4f5895f3-2d93-4355-bcbb-97c6701d0028\" name=\"Changes\" comment=\"\" />\r\n      <created>1703141547529</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1703141547529</updated>\r\n      <workItem from=\"1703141550763\" duration=\"1349000\" />\r\n      <workItem from=\"1703142922707\" duration=\"7847000\" />\r\n      <workItem from=\"1704341315070\" duration=\"7122000\" />\r\n      <workItem from=\"1704766847308\" duration=\"10108000\" />\r\n      <workItem from=\"1705028553447\" duration=\"9551000\" />\r\n      <workItem from=\"1705287635684\" duration=\"4323000\" />\r\n      <workItem from=\"1705292066581\" duration=\"3243000\" />\r\n      <workItem from=\"1705295667683\" duration=\"2883000\" />\r\n      <workItem from=\"1705633485826\" duration=\"9982000\" />\r\n      <workItem from=\"1705889164536\" duration=\"2931000\" />\r\n      <workItem from=\"1705967371702\" duration=\"1489000\" />\r\n      <workItem from=\"1706062032285\" duration=\"1298000\" />\r\n      <workItem from=\"1706259312347\" duration=\"426000\" />\r\n      <workItem from=\"1706497270526\" duration=\"5894000\" />\r\n      <workItem from=\"1706585641698\" duration=\"1168000\" />\r\n      <workItem from=\"1706670760232\" duration=\"3283000\" />\r\n      <workItem from=\"1707102402558\" duration=\"1863000\" />\r\n    </task>\r\n    <servers />\r\n  </component>\r\n  <component name=\"TypeScriptGeneratedFilesManager\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\r\n    <SUITE FILE_PATH=\"coverage/ASX$Stock_Data_Visualisation.coverage\" NAME=\"Stock_Data_Visualisation Coverage Results\" MODIFIED=\"1704341453752\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/ASX$Stock_Market_Application.coverage\" NAME=\"Stock_Market_Application Coverage Results\" MODIFIED=\"1705983449820\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision c196cf49e467f6b5f5ce0aa5db7f273c1715d3b7)
+++ b/.idea/workspace.xml	(date 1710222391735)
@@ -4,21 +4,14 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="4f5895f3-2d93-4355-bcbb-97c6701d0028" name="Changes" comment="">
-      <change afterPath="$PROJECT_DIR$/.idea/ASX.iml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/modules.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/other.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+    <list default="true" id="4f5895f3-2d93-4355-bcbb-97c6701d0028" name="Changes" comment="Add stock market analysis &#10;&#10;Scrape stock data using yfinance API.&#10;Add outlier detection and metadata.">
       <change afterPath="$PROJECT_DIR$/Building a Dash app.md" afterDir="false" />
       <change afterPath="$PROJECT_DIR$/Data Structures of Stock Data.py" afterDir="false" />
       <change afterPath="$PROJECT_DIR$/Stock Analysis.md" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/Stock_Data_Analysis.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/Stock_Market_Application.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/main.py" afterDir="false" />
       <change afterPath="$PROJECT_DIR$/stock_market_app_review.md" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/ASX.iml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/ASX.iml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Stock_Data_Analysis.py" beforeDir="false" afterPath="$PROJECT_DIR$/Stock_Data_Analysis.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -61,24 +54,26 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "Python.Stock_Data_Visualisation.executor": "Run",
-    "Python.Stock_Market_Application.executor": "Run",
-    "RunOnceActivity.OpenProjectViewOnStart": "true",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "SHARE_PROJECT_CONFIGURATION_FILES": "true",
-    "git-widget-placeholder": "master",
-    "last_opened_file_path": "C:/Users/laran/PycharmProjects/ASX",
-    "node.js.detected.package.eslint": "true",
-    "node.js.detected.package.tslint": "true",
-    "node.js.selected.package.eslint": "(autodetect)",
-    "node.js.selected.package.tslint": "(autodetect)",
-    "nodejs_package_manager_path": "npm",
-    "settings.editor.selected.configurable": "reference.settingsdialog.IDE.editor.colors.VCS",
-    "vue.rearranger.settings.migration": "true"
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;Python.Stock_Data_Visualisation.executor&quot;: &quot;Run&quot;,
+    &quot;Python.Stock_Market_Application.executor&quot;: &quot;Run&quot;,
+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;SHARE_PROJECT_CONFIGURATION_FILES&quot;: &quot;true&quot;,
+    &quot;ToolWindowPlots.ShowToolbar&quot;: &quot;false&quot;,
+    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
+    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,
+    &quot;last_opened_file_path&quot;: &quot;C:/Users/laran/PycharmProjects/ASX&quot;,
+    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
+    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
+    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
+    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
+    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;,
+    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
   }
-}]]></component>
+}</component>
   <component name="RunManager">
     <configuration name="Stock_Market_Application" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="ASX" />
@@ -140,15 +135,58 @@
       <workItem from="1706497270526" duration="5894000" />
       <workItem from="1706585641698" duration="1168000" />
       <workItem from="1706670760232" duration="3283000" />
-      <workItem from="1707102402558" duration="1863000" />
+      <workItem from="1707102402558" duration="7450000" />
+      <workItem from="1707192362617" duration="1247000" />
+      <workItem from="1707274948079" duration="3437000" />
+      <workItem from="1707707791643" duration="4132000" />
+      <workItem from="1707879719970" duration="917000" />
+      <workItem from="1707971845089" duration="1270000" />
+      <workItem from="1708030997616" duration="1466000" />
+      <workItem from="1708398047783" duration="3358000" />
+      <workItem from="1708570867626" duration="6436000" />
+      <workItem from="1708676380820" duration="2000" />
+      <workItem from="1708739543237" duration="677000" />
+      <workItem from="1709002760714" duration="13655000" />
+      <workItem from="1709066117033" duration="4659000" />
+      <workItem from="1709175398565" duration="10042000" />
+      <workItem from="1709186105985" duration="663000" />
+      <workItem from="1709590843052" duration="4245000" />
+      <workItem from="1709614088259" duration="3920000" />
+      <workItem from="1709780598640" duration="10056000" />
+      <workItem from="1709796875248" duration="18000" />
+      <workItem from="1710215798572" duration="5209000" />
+    </task>
+    <task id="LOCAL-00001" summary="Add stock market analysis &#10;&#10;Scrape stock data using yfinance API.&#10;Add outlier detection and metadata.">
+      <option name="closed" value="true" />
+      <created>1707104517108</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1707104517108</updated>
     </task>
+    <option name="localTasksCounter" value="2" />
     <servers />
   </component>
   <component name="TypeScriptGeneratedFilesManager">
     <option name="version" value="3" />
   </component>
+  <component name="VcsManagerConfiguration">
+    <MESSAGE value="Add stock market analysis &#10;&#10;Scrape stock data using yfinance API.&#10;Add outlier detection and metadata." />
+    <option name="LAST_COMMIT_MESSAGE" value="Add stock market analysis &#10;&#10;Scrape stock data using yfinance API.&#10;Add outlier detection and metadata." />
+  </component>
+  <component name="XDebuggerManager">
+    <breakpoint-manager>
+      <breakpoints>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/Stock_Data_Analysis.py</url>
+          <line>3</line>
+          <option name="timeStamp" value="1" />
+        </line-breakpoint>
+      </breakpoints>
+    </breakpoint-manager>
+  </component>
   <component name="com.intellij.coverage.CoverageDataManagerImpl">
     <SUITE FILE_PATH="coverage/ASX$Stock_Data_Visualisation.coverage" NAME="Stock_Data_Visualisation Coverage Results" MODIFIED="1704341453752" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/ASX$Stock_Market_Application.coverage" NAME="Stock_Market_Application Coverage Results" MODIFIED="1705983449820" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/ASX$Stock_Market_Application.coverage" NAME="Stock_Market_Application Coverage Results" MODIFIED="1709614658811" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
   </component>
 </project>
\ No newline at end of file
Index: stock_market_app_review.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/stock_market_app_review.md b/stock_market_app_review.md
new file mode 100644
--- /dev/null	(date 1709796892153)
+++ b/stock_market_app_review.md	(date 1709796892153)
@@ -0,0 +1,597 @@
+
+23/12/2023
+**Things I learned today**
+- A list can store DataFrames
+- It is generally a good practice to use lists or dictionaries to store variables since they give us more 
+functionality and flexibility
+- We can create a for loop to go through all of the stock market
+DataFrames, reset Date indices to be columns and create new variables
+- The enumerate function goes through the list and tracking its iteration
+
+4/1/2024
+**Things I learned today**
+- Refactoring: process of restructuring existing computer code without changing its external behavior. 
+Goal: to improve the design, structure,and/or implementation of the software (its non-functional attributes) while preserving its
+functionality 
+Benefits: improve code readability, and reduced complexity (can enhance the source code's maintainability, creating simpler, cleaner and more
+expressive internal architecture or object model to improve extensibility, discover and fix hidden or dormant bugs or 
+vulnerabilities in the system since it simplifies the underlying project and diminishes unnecessary levels of complexity). 
+Refactoring is 1 of the primary means of repaying technical debt
+- Error message: TypeError: unhashable type :'DataFrame' normally happens when we try to use a pandas DataFrame (which is
+mutable, hence unhashable) +. using DataFrame as a dictionary ky or in a set, inccorect usage of .loc or iloc, passing DataFrame to a function
+expecting a hashable type
+
+*** Things I did well ***
+- Figuring out a way to write a function to perform some analysis on multiple dataframes 
+
+*** Things I did not do well ***
+- Wrote a function to perform some analysis on multiple dataframes that did not work
+
+*** Things I will do better next time ***
+- Ask Bing for some idea
+- Understand why it did not work 
+
+9/1/2023
+***Things I learned today:***
+- zip (): a built-in function taking 2 or more iterables and returns an iterator 
+aggregating elements from each iterable. The resulting iterator contains tuples, where
+the i-th tuple contains the i-th element from each of the input iterables. 
+- plotly.graph_objs: a module in plotly providing a set of classes for creating and manipulating figures. 
+We call these "graph objects" and represent parts of a figure. 
+- No def needed when creating a plot. Simply no need to. 
+- Plotly is a data visualisation library allowing you to create interactive plots and 
+charts in Python. Advantages: interactive visualisations, ease of use, customisation, compatibility,
+community support. In general, if your data set is small and simple, and you only need to communicate a few key insights, a static visualization might be the best choice. 
+However, if your data set is large and complex, and you need to allow the user to explore the data in more detail, an interactive visualization might be a better choice.
+- A Scatter object: used to create line charts, scatter charts, text charts and bubble charts. 
+***Things I did well today:***
+- Understanding how to write a function to take 2 or more iterables and return a iterator aggregating
+elements from each iterable using zip. The function takes 2 lists from stock data and stock names to print information, shape, data types and statistical descriptions. 
+- Understanding how to write a function to reset index of DateTimeIndex to a column of stock data using enumerate. 
+- Learning to use Plotly to create an interactive plot since it has advantages of interactive
+visualisations, ease of use, customisation, compatibility and community support. When the dataset is complex and large, we can use it.
+
+***Things I didn't do well today:***
+- When visualising stock data Price vs. Time, there were multiple plots created, not just 1 plot needed.
+- Didn't write the code completely off my head
+
+*** Things I will do better next time: ***
+- Chunking code in memory and recreate it 
+- Looking into why go.Scatter created multiple plots instead of one 
+
+12/1/2024
+***Things I learned today***
+- Building a Dash application requires 2 parts: the first one is the layout of the app, describing what the app looks like, 
+the 2nd part describe the interactivity of the app
+***Things I did well today ***
+- Reconstructing the code to reset all of DateTimeIndex for all stock data from memory
+- Learning how to build a Dash app
+***Things I did not do well today ***
+- Unable to build the Dash app using dcc.Graph (deprecated)
+***Things I will do better next time ***
+- Figure out how to build a Dash app using dcc.Graph, perhaps checking with Amin or JetBrain support team 
+
+15/01/2024
+***Things I learned today ***
+- In Dash @callback, 'my-id' and 'value' correspond to the unique identifier and current value of the component, which needing lookup
+- In @callback, 'inputs' and 'outputs' are arguments of callback decorator. Whenever an input changes, the function the callback decorator wraps around will change automatically
+- To detect outliers in stock data, we can use the method of Interquartile Range (IQR). It is the range between the first quartile (25 percentile) and the second quartile (75 percentile).
+Any data points fall below the Q1 - 1.5*IQR and above the Q2 + 1.5*IQR will be outliers.
+***Things I did well today***
+- Figuring out how to build an interactive Dash visualisation app
+- Figuring out how to conduct Exploratory Data Analysis
+- Detecting outliers using IQR method
+
+*** Things I did not do well today ***
+- Not getting the dash app to work
+- Taking too much time with the dash app
+
+*** Things I will do better next time ***
+- Only spend 1 hour on Dash app. If not working, make notes to ask
+- Spend more time on data analysis 
+
+19/01/2024
+***Things I learned today***
+- if plt.show() outside the loop, only show the box plot last iteration (pypl). If inside, show all the box plots
+- Reasons for outliers in stock market data: market volatility, significant news or events, earnings reports, changes in 
+market sentiments, and data entry errors
+- Outliers, though often viewed as 'data problems' we need to fix, may offer interesting explanations that can lead to 
+theoretical insights since we can study them as unique phenomena. Therefore, we need to develop clear guidelines and a better understanding
+in 3 areas: a) how to define them, b) how to detect them, c) how to handle them. 
+
+***Things I did well today***
+- Writing clearer, concise and higher quality code to replace the old one
+- Getting better at understanding the stock market data
+- Looking into outliers, reasons for their existence and how to handle them
+
+***Things I did not do well today***
+- Not understanding how to handle outliers in stock market data yet
+- Talking to Hayden while doing deep work
+
+***Things I will do better next time***
+- Asking Hayden to go out for a walk while doing deep work or simply moving to the library
+- Better grasp how to handle outliers in stock market data
+
+
+22/01/2024
+*** Things I learned today ***
+- Considerations about outliers: outliers can lead to important changes in parameter estimates when researchers 
+use statistical methods relying on maximum likelihood estimator and how we deal with outliers
+may lead us to falsely accept or reject hypotheses; we must use caution since in most cases, deleting 
+outliers helps us support our hypotheses and opting a course of action post hoc that is certain to increase
+our likelihood of finding what we want to find is a dangerous practice.
+- The decisions researchers make about how to define, identify, and handle outliers have
+crucial implications, because such decisions change substantive conclusions.
+- 14 outlier definitions: single construct outlier, error outlier, interesting outlier, discrepancy outlier,
+model fit outlier, prediction outlier, influential meta-analysis effect size outlier, influential meta-analysis
+sample size outliers, influential meta-analysis effect and sample size outlier, cluster analysis outlier, influential
+time series additive outlier, influential series innovation outliers, influential level shift outlier,
+influential temporary changes outlier
+
+***Things I did well today ***
+- Reading and scanning papers about outliers
+- Digging deeper into outliers
+
+*** Things I did not do well today ***
+- Taking more time to read. Should take about 3 - 4 hours to read
+
+*** Things I will do better next time ***
+- Speeding up my reading, perhaps can skim some information
+
+24/01/2024
+***Things I learned today***
+- outlier identification techniques: box plot, stem and leaf plot, schematic plot analysis,
+standard deviation analysis, percentage analysis, scatter plot, q-q plot, p-p plot, standardised residual,
+studentised deleted residual, euclidean distance, mahalanobis distance, K-clustering,
+2- or 30 dimesional plots of the original and PCA, autocorrection function plot, time plot, extreme studentised deviate,
+Hosmer and Lemeshow goodness-of-fit test, Leverage value, centred leveraged value, deletion standardised multivariate residual,
+Cook's Dt, Modified Cook's Dt, Generalised Cook's Dt,difference in fits (standardised), 
+difference in beta (standardised), Chi-squared difference test, single parameter influence, average squared 
+deviation technique, sample-adjusted meta-analytic deviancy, conduct anaysis with and without outliers, nearest neighbor techniques,
+nonparametric methods, semiparametric methods, iterative outlier identification procedure, independent component analysis
+
+***Things I did well today***
+- Understanding outlier detection techniques in detail
+
+***Things I did not do well today***
+- Slow speed in reading the paper
+
+***Things I will do better next time***
+- Using abbrev, shortening sentences, copying 
+
+***Things I learned today***
+- Standardised residual: calculated by dividing the ith observation's residual value by a std deviation term
+- Studentised residual: measuring both the outlyingness of the observation regarding its standardised residual
+value and the outlyingness of the observation on the predicted value. 
+- Standardised deleted residual: the predicted value for the focal observation is calculated without the observation itself
+- Euclidean distance: length of the line segment between 2 specified points in different dimension space, can be calculated
+from the Cartesian coords of the points using the Pythagorean theorem. 
+- Mahalanobis distance: similar to Euclidean distance, the length of the line segment between a data point and the centroid
+of the remaining cases, where the centroid is the point created at the intersection of the means of all the prediction vars.
+A large Mahalanobis distance may mean the corresponding observation is an outlier. 
+- K-clustering: yield different candidate subsets that then have to be evaluated by 1 or more multiple-case diagnostics
+- 2 - or 3-dimensional plots of the original and the principal component vars: produced as a result of a principal 
+component analysis. A isolated data point may indicate a potential outlier.
+- autocorrelation plot: computing autocorrelations for data valyes at varying time lags. 
+- time plot: relationship between a certain variable and time.
+- extreme studentised deviate: different btwn a var's mean and query value, / a std value
+- Hosmer and Lemeshow goodness-of-fit test: a Pearson chi-square stat from a table of observed and expected freq.
+
+***29/01/2024***
+***Things I did well today***
+- Identifying outliers and factors might cause them
+- Looking at different methods and techniques to detect outliers
+
+***Things I did not do well today***
+- Taking some time to read the paper
+
+*** Things I will do better next time***
+- Give myself more time to read
+
+***31/01/2024***
+
+***Things I did well today***
+- To calculate regression coefficients:
+1. Fit a linear regression model to our data.
+2. Calculate the regression coefficients using:
+The coefficient of X is given by the formula a = n(∑xy)−(∑x)(∑y) / n(∑x2)−(∑x)2
+The constant term is given by the formula b = [(∑y)(∑x^2)−(∑x)(∑xy)] / [n(∑x2)−(∑x)2]
+3. Lastly, we can input the regression coefficients in the equation Y = aX + b to obtain
+the predicted vals of Y. 
+- Cook's Di: assessing the influence a data point i has on all regression coefficients as a whole, 
+calculated by deleting the observation from the dataset and then recalculating the regression coefficients. 
+Di = [(ri^2)/(p*MSE)]/[(hii)/(1-hii)^2], where ri denotes the ith residual, p is the number of coefficients in the 
+regression model, MSE is the mean sqrd error, hii is the ith leverage value
+- Modified Cook's Di: using standardised deleted residuals rather than standardised residuals
+Di* = (Di)/(p*MSE) * (hii)(1-hii)^2, where Di: Cook's distance, p is the number of coefficients in the regression model,
+MSE is the mean squared error, hii is the ith leverage val.
+Taking into acc the number of independent vars in the model. 
+- Generalised Cook's Di: applied to structural equation modelling to assess the influence a data point has on
+the parameter estimates.
+Di^* = ri^2/(p * MSE *hii), where rii denotes the ith residual, p is the number of coeffs in the regression model,
+MSE is the mean squared error, hii is the ith leverage val.
+- Structural equation modelling: method used to analyse the relationships between the observed and latent vars. Combining
+factor analysis and multiple regression analysis. We create a model representing how various aspects of some phenomenon
+thought to causally connect to one another. Often contain postulated causal connections among some latent vars. Additional
+causal connections link those latent vars to observed vars. 
+- latent vars: vars we cannot directly observe or measure, but inferred from other observable vars that can be directly observed
+or measured via a mathematical model, representing underlying concepts or construct we cannot directly measure.  
+- Difference in fits, standardised (DFFITSi): assessing the influence a data point i has on all regression coeffs as a whole,
+Diff between Cook's Di and this is that they produce info on different scales. A diagnostic measure to detect influential 
+observations, measuring the diff in the predicted val for a point, obtained when that point is left out of the regression.
+DFFITS is the Studentised DFFIT where we can achieve studentisation by dividing the estimated std of the fit at that point.
+DFFITSi = (^yi - ^y(i))/(sqrt(MSEi x hii)), where ^yi denotes the predicted val for the ith observation, 
+^y(i) the predicted val for the ith obs when we exclude it from the regression, MSE(i) the mean sqrd error when the 
+ith obs is excluded from the regression, hii the ith leverage value 
+Obs with DFFITS val greater than 2x the sqr root of the no of parameters / the no of obs are considered to be influential 
+
+- Difference in beta, standardised (DFBETASij): assessing if the inclusion of a case i leads to an increase or decrease in a single regression coeff
+j i.e. a slope or intercept. Measuring the difference in the estimated regression coeff for a predictor car when that obs is included or excluded 
+from the regression model. 
+DFBETAS(ij) = [beta_i - beta(i(-j)]/[sqrt(MSE(-i) x hii))]
+
+- Chi-squared difference test: allowing a researcher conducting SEM to assess the diff in the model
+fit between 2 models, 1 with the outlier and the other without 
+To conduct a chi-squared difference test:
+1. Define a null hypo and the alternative hypo
+2. Calculate the expected frequencies for each category under the null hypo
+3. Need to calculate the chi-squared test stat using this:
+chi^2 = sum{i=1}^{k}\frac(O_i = E_i)^2}{E_i}
+where:
+O_i is the observed frequency for the category i
+E_i is the expected frequency for category i
+k is the number of categories
+4. Need to compare the calculated chi-squared test stat to the critical value of the 
+chi-squared distribution with k-1 degrees of freedom. If the test sta > 
+the critical val => reject the null hypothesis to conclude that there is a sig diff 
+between the observed and expected frequencies of the categorical var.  
+- Single parameter influence: used in SEM to assess the effect of an outlier on a specific parameter 
+estimate, as opposed to the overall influence of an outlier on all para estimates
+Identifying which parameters have the greatest impact on the model's output. We can calculate
+using various methods: DIFFITS, DFBETAS, and Cook's distance. 
+A common rule of thumb: observations with a DIFFITS val greater than 2 x the sqr ropt of the no of 
+parameters/the no of obs considered to be influential. Also, points with a large Cook's distance are considered
+influential. 
+- Avg squared deviation technique: when conducting multilevel modeling, explores the effect each group
+has on the fixed &/or random parameters, allowing for the identification of higher-level prediction outliers. 
+Measuring the variability of a set of data points. 
+Calculated by finding the diff between each data point & the mean if the data set, squaring the differences, adding 
+them together, and dividing by the no of data pts - 1. 
+s^2 = (sum_{i_1}^{n}({x_i - \bar{x})^2/{n-1}
+where:
+s^2: the avg sqrd deviation 
+s_i is the ith data pt
+x_i: the ith data pt
+\bar{x} is the mean of the data set
+n : the no of data pts
+
+A large avg sqrd deviation: indicates the data pts are more spread out, while a small avg sqrd deviation conveys data points are tightly clustered around 
+the mean. 
+
+
+- ***Things I did well today***
+- Reading in-depth about outlier identification techniques
+
+***Things I did not do well today***
+- Woke up a bit late to study
+***Things I will do better next time***
+- Have an alarm ready
+
+***05/02/2024***
+***Things I learned today***
+- Sample-adjusted meta-analytic deviancy (SAMD): in metaanalysis, it takes the diff between the val of each primary-level
+effect suze estunate and the mean sample-weighted coeff  computed without that effect size in the analysis, then alternates 
+the diff val based in the sample size of the primary-level study. Can use SAMD to detect outliers. Helps us to identify
+studies whose effect sizes deviates significantly from the overall trend. Calculates external studentised residuals for each study.
+By considering the sample size, SAMD gives us a more robust way to detect outliers and helps researchers identify studies that might be driving 
+unusual patterns in the meta-analysis. 
+- Conduct analysis with or without outliers: when results differ across the 2, we confirm that the data pt is indeed an outlier. Why:
+sensitivity check, transparency, understanding impact.
+How: document, describe approach, report the diffs
+- Nearest Neighbor techniques: calculate the closest val to the query val using various types of distance metrics. 
+Techniques: KNN, optimised NN, 1-NN, 2-NN, NN with reduced features, dragon method, PAM (partitioning around medoids),
+CLARANS (clustering large apps based on randomised search) and graph connectivity method.
+
+- Nonparametric methods: fitting a smoothed curved without making any constraining assumptions about the data. Lack of 
+linear relationships indicates the presence of outliers. Not relying on specific assumptions about the parameters of the
+data distribution. Often used when the assumptions of the parametric methods are violated. 
+Charateristics: distribution-free, function on samples (defining stats as functions on samples dependency on specific parameters). 
+Can use them for descriptive stats, statistical inference, modelling financial time series. 
+- Semiparametric methods: combining the speed and complexity of parametric methods with the flexibility of nonparametric methods
+to understand local clusters or kernels rather than a single global distribution model. We identify outliers as lying in regions of 
+low density. Striking the balance of flexibility and structures.  
+Components of semiparametric models: 
+parametric components (representing a finite-dimensional vector) & nonparametric components (representing a infinite-dimensional vector)
+Advantages: flexibility, robustness, interpretability/ 
+Challenges: estimation, curse of dimensionality
+ApplicationL biostats, econs, env sciences
+- Iterative outlier identification procedure: allowing for the estimate of the residual std to identify
+data pts sensitive to the estimation procedure used for a time series analysis. 
+How:
+Identify potential outliers
+Remove or adjust outliers
+Recompute stats
+Repeat step 1-3
+- Independent component analysis: separate independent component by maximising the 
+statistical independence among them. We identify the separate independent components 
+as outliers. 
+Objective: separate a multivariate signal into additive subcomponents, 
+assuming at most subcomponent is Gaussian (normal), and the subcomponents are statistically independent
+from each other. 
+Goal: unmix and decompose signal into its original independent sources
+Key assumptions: source signals are independent of each other. Values in each source signal
+have non-Gaussian distribution
+
+Application: cocktail party problem (separate overlapping speech signals from 
+multiple speakers), blind source separation (extracting individual sources from their
+mixture), biomedical signal processing (separating EEG signals, fMRI data), image processing
+(texture analysis, face recognition). financial data (separating market signals)
+Mathematical approach: attempt to find a linear transformation of the data such that 
+the xformed components are independent as possible. works well when the statistical 
+independence assumption holds.
+ICA goes beyond PCA by seeking statistically independent components
+
+A good way to disentangle complex mixture and reveal hidden structures in data
+
+***Things I did well today***
+- Understanding outlier detection methods more in-depth
+- Be more on-time 
+- More focus
+- Identifying the Iterative outlier identification procedure as a suitable procedure for this project
+***Things I did not do well today***
+- Syntax error with using ' ' instead of " " after f
+- Not completing the analysis
+
+***Things I will do better next time***
+- using f " "
+- figuring out more ways to conduct analysis to make it more complete
+
+
+*** 7/2/2024***
+***Things I learned today***
+- Correct val: correcting a data pt to its proper val.
+- Remove outlier: eliminate the data pt from the analysis.
+- Study the outlier in detail: conduct follow-up work to study the case
+as a unique phenomenon of interest. Analyse outlier, study its impact
+- Keep: acknowledge the presence of an outlier, but do nothing prior to the
+analysis
+- Report findings with and without outliers: report substantive results with 
+and without, include any explanation for any diff in the results. Be transparent.
+- Winsorisation: transforming extreme vals to a specified percentile of the data
+- Truncation: setting observed vals within a certain range, anything outside we will
+remove
+- Transformation: applying a deterministic mathematical function to each val.
+- Modification: changing an outlier to another val, less extreme one.
+- Least absolute deviation: similar to ordinary least squares (method to estimate the 
+unknown parameters in regression). Better than LSD when errors follow non-Gaussian distribution with longer tails. 
+- Least trimmed squares: ordering the squared residual for each case from the highest to the lowest,
+then trim or remove the highest val. 
+- M-estimation: a class of robust techniques reducing the effect of influential outliers
+by replacing the squared residuals by another func of them. 
+- Bayesian stats: obtaining parameter estimates by weighing prior info and the observed data at hand
+- 2-stage robust procedure: use Mahalanobis distance to assign weights to each data pt, extreme cases
+are downweighted. completed via a 2-stage process.
+
+***Things I did well today***
+- Learning more about how to handle outliers: correct val, remove outlier. study the outlier in detail,
+keep, report findings with or without the outlier, winsorisation, truncation, transformation, modification, 
+least absolute deviation, least trimmed squares, M-estimation, Bayesian stats, 2-stage robust procedure
+
+***Things I did not do well today***
+- Not writing any code
+
+***Things I will do better next time***
+- Writing some code: will do some PCA maybe? 
+
+***12/02/2024***
+***Things I learned today***
+- Direct robust method using iteratively reweighted least squares: using Mahalanobis distance to
+assign weights to each data pt. We complete the assignment of weights via using an iteratively reweighted least 
+squares algo. IRLS also commonly used for robust regression, especially when dealing with outliers or heavy-tailed 
+error distributions. IRLS aim to mitigate the impact of outliers by downweighting their influence during parameter 
+estimation. IRLS updates the regression coeffs by reweighting the observations based on their residuals: initialisation, 
+weight calculation, re-estimation, converge check, final estimates. 
+- generalised estimating equations (GEE) methods: estimating the variances and covariances in the random part of the 
+multilevel model directly from the residuals. Useful for analysing correlated data, allowing to acc for dependencies and estimate pop-up effects
+- Multilevel models: statistical techniques used to analyse data with a hierarchical or nest structure. Level 1 model: describe indv-level relationships. 
+Level-2 model: describe group-level relationships
+- Boostrapping methods: estimate parameters of a model and their standard errors from the sample, without reference to a theoretical sampling distribution. 
+App: statistical inference, regression models, ML
+- Non-parametric methods: does not assume the data distributed in any particular way
+- Unweighted meta-analysis: obtaining meta-analytic stats not giving more weight to primary level studies with large sample sizes. How: each 
+study contributes equally to the overall effect estimate.
+- Generalised M-estimation: a class of robust techniques reducing the effect of outliers by replacing the squared residuals by another func of the residuals.
+- 3 shortcomings authors identify: 1st => it is common for organisational science researchers to either vague or not transparent on how outliers are defined
+and how a particular outlier identification method chosen and used, 2nd => we identify outliers in one way but then used another outlier identification technique not 
+congruent with their adopted outlier definition, 3rd => the authors found little discussion, let alone recommendations, on the subject of studying outliers interesting 
+and worthy of further examination. 
+- A pervasive view: outliers are problems that we should fix
+- 2 principles: we should describe choices and procedures regarding the treatment outliers that we have implemented, and we should clearly and explicitly acknowledge the type
+of outlier where they are interested and use an identification technique congruent with the outlier definition
+
+***Things I did well today***
+- Learning about: direct robust method using iteratively reweighted least squares, generalising estimating equations methods, boostrapping methods, non-parametric method, unweighted 
+meta-analysis, generalised M-estimation, 3 shortcomings, 2 principles
+***Things I did not do well today***
+- Not defining an outlier yet***
+*** Things I will do better next time***
+- Defining outliers
+
+***13/02/2024***
+***Things I learned today***
+- 3 types of outliers: error outliers, interesting outliers and influential outliers
+- 2 steps of detecting error outliers: 1st step => locating outlying obs, 2nd => separately 
+investigating whether the outlyingness of such data pts was caused by errors
+- 2 categories of techniques to identify error outliers: single construct and multiple construct
+
+***Things I did well today***
+- Learned about types of outliers, steps of detecting error outliers and 2 categories to 
+identify error outliers
+
+***Things I did not do well today***
+- Did not study for the whole 3 hours due to tiredness and tummyache
+
+***Things I will do better next time***
+- Anticipate beforehand to fix my schedule
+
+***16/02/2024***
+***Things I did well today***
+- Single construct techniques: refer to the measurement of constructs expected
+to have a single underlying dimension. 2 steps: conceptualisation and operationalisation. 
+The recommendation is using visual tools first then follow up with quantitative approaches. Can use 
+recommended cutoffs. 
+- Multiple construct techniques: use them when we believe a concept or construct
+to have multiple dimensions, using multiple measures or tests to capture the different
+dimensions of a construct. 2 steps are similar to single construct techniques. Use
+recommended cutoffs when applicable. 
+- Construct: abstract concepts not directly observable
+- Researchers should keep diaries, logs or journals during data collection to use 
+retrospectively to decide if something unusual happened with some particular case that 
+they can no longer trace after the fact. 
+- The 2nd step in the process of understanding the possible presence of outliers is 
+examining interesting outliers. Do not automatically treat any outlying data pts as harmful. 
+- Interesting outliers: accurate data pts, identified as outlying obs ( potential error outliers)
+but not confirmed as actual error outliers. These cases may contain potentially valuable or unexpected knowledge
+- Identifying interesting outliers involve 2 steps: identifying potentially interesting outliers and
+identifying which outliers are indeed interesting. 
+- Pursuing potential interesting outliers likely will include the examination of 
+a great deal of error outliers going undetected as errors.
+
+***Things I did well today***
+- Focusing on my study of detecting and handling error outliers and interesting outliers
+
+***Things I did not do well today***
+- Talking to Hayden during deep work
+
+***Things I will do better next time***
+- Informing Hayden about my deep work session
+
+***20/02/2024***
+***Things I learned today***
+- We address influential outliers differently from error outliers and interesting outliers depending 
+on particular statistical techniques. 
+- 2 types of influential outliers: model fit (presence changes the fit of the model) and prediction (change the parameter estimates of the model).
+- 2 step process to identify model fit outliers: identifying data pts most likely to have influence
+on the fit of the model since they deviate markedly from other cases in the dataset, investigating 
+these cases to understand if they genuinely have influence on the model fit. 
+- 3 techniques to assess the presence of pred outliers in regression: DIFFITS, DFBETAS, Cook's Di
+- 3 courses of action to handle model fit and pred outliers: respecification (adding other terms to the regression equation,]
+deletion and robust approaches (involving non-OLS standard). We should always report results with 
+and without the technique.
+- the process of identifying model fit outliers in SEM is similar to regression
+- In regression, there are 2 types of pred outliers: global (impact all parameter estimates of the model) 
+and specific (impacting 1 parameter estimate)
+- We should use gCDi stat to detect global pred outliers while not neglecting the detection
+of specific pred outliers (using the standardised change in the jth parameter resulting 
+from the deletion of the obs i)
+- Handling influential outliers in SEM is similar to in regression. Recommended the use of deletion
+or robust approaches
+- In multilevel modelling, the main goal of an analysis is assessing the size of the variance
+components and the sources of such variances. 
+- The recommendation is a top-down approach in identifying model fit outliers in multilevel modelling,
+beginning at the highest level of analysis. The researcher then can decide whether a group of obs
+affects the model fit b/c of a) the group itself and/or b) a particular data pt(s) in the group
+- In multilevel modelling, we use a top-down approach. The recommendation is calculating 
+the avg sqrd deviation, then the researcher can compare Cj vals against one another using an index plot. 
+
+***Things I did well today***
+- completing correlation heat maps
+- Learning about: influential outliers and how to handle them in regression, SEM and multilevel modelling
+
+***Things I did not do well today***
+- Not finishing the paper on identifying and handling outliers
+
+***Things I will do better next time***
+- Finishing the paper
+
+***22/2/2024***
+***Things I learned today***
+- Handling influential outliers in multilevel modeling are similar to those used in regression and SEM. But, unlike regression,
+researchers need to 1st decide the level where any additional predictor(s) are to be added in the multilevel modelling context
+- Options for robust techniques in multilevel modeling include: generalised estimating equations (GEE) and bootstrapping methods
+
+***Things I did well today***
+- Finishing the paper
+- Code to visualise Q-Q plots 
+
+***Things I did not do well today***
+- Not reading other papers about outliers 
+- Not doing time series analysis
+
+***Things I will do better next time***
+- Reading more papers about outliers
+- Doing time series analysis
+
+***27/02/2024***
+***Things I learned today***
+- ValueError: an exception raised when a function receives an argument of the correct type but an inappropriate value. 
+- AttributeError: an exception raised when an attribute reference or assignment fails, normally occurs when
+trying to access or modify an attribute or method that doesn't exist for a specific object or class 
+- In Python, flattening an array means transforming a multi-dimensional array to a one-dimensional array to make it 
+easier to iterate over
+- We can ask Copilot to make our code more concise
+
+***Things I did well today***
+- Coding Q-Q Plot
+- Coding distribution tests
+
+***Things I did not do well today***
+- Not being able to code box plots 
+
+***Things I will do better next time***
+- Coding box plots for all columns
+
+***29/02/2024***
+***Things I learned today***
+- https://www.machinelearningplus.com/plots/matplotlib-plotting-tutorial/
+- KeyError: occuring when trying to access a key that isn't in a dictionary or dictionary-like object
+- TypeError: occuring when types don't match when performing Python operations
+- to draw a line plot using matplotlib, ordinals have to meet the requirements
+
+***Things I did well today***
+- Trying to use matplotlib to draw line plots
+- Using plotly to draw line plots 
+
+***Things I did not do well today***
+- Encountering problems with matplotlib
+
+***Things I will do better next time***
+- Fixing matplotlib
+
+***5/3/2024***
+***Things I learned today***
+- If the error "Permission denied" => no access to Python packaging tools => need to add the variable to PATH. Also need to create a different
+venv to test
+- decomposition models can be additive or multiplicative. Additive: when seasonality and irregular variations don't change as much as trends changes.
+Multiplicative: when seasonality and irregular variations increase in amplitude as trend changes
+- When different features have different scales: consider adding another y-axis or normalise the data
+
+***Things I did well today***
+- Fixing matplotlib
+
+***Things I did not do well today***
+- Not finishing with visualising all features, including Volume with plotly
+- Not finishing with seasonal.decompose yet 
+***Things I will do better next time***
+- Visualising all features with a line plot
+- Finishing with seasonal.decompose
+
+***7/3/2024***
+***Things I learned today***
+- Multivariate normality: when a dataset has multiple variables and these variables have data points
+that are normally-distributed together. To assess whether multivariate normality exist: visual inspection (scatter plot
+for pair vars to see if there are signs of elliptical patterns), PCA (checking variance), Mardias's test
+- PCA: identify trends and relationships, help with dimensionality reduction and feature extraction
+
+***Things I did well today***
+- Drawing scatter plots for variables to see their relationships
+- Checking for multivariate normality 
+- Performing PCA
+
+***Things I did not do well today***
+- Spending a lot of time with Shapiro-Wilk test programming with stock data when not knowing whether it is right for them
+- Not finishing multivariate normality check
+
+***Things I will do better next time***
+- Checking the business problem at hand
+- Finishing multivariate normality check and understanding why multivariate normality
\ No newline at end of file
