Index: misc/Stock App Review.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\n23/12/2023\r\n**Things I learned today**\r\n- A list can store DataFrames\r\n- It is generally a good practice to use lists or dictionaries to store variables since they give us more \r\nfunctionality and flexibility\r\n- We can create a for loop to go through all of the stock market\r\nDataFrames, reset Date indices to be columns and create new variables\r\n- The enumerate function goes through the list and tracking its iteration\r\n\r\n4/1/2024\r\n**Things I learned today**\r\n- Refactoring: process of restructuring existing computer code without changing its external behavior. \r\nGoal: to improve the design, structure,and/or implementation of the software (its non-functional attributes) while preserving its\r\nfunctionality \r\nBenefits: improve code readability, and reduced complexity (can enhance the source code's maintainability, creating simpler, cleaner and more\r\nexpressive internal architecture or object model to improve extensibility, discover and fix hidden or dormant bugs or \r\nvulnerabilities in the system since it simplifies the underlying project and diminishes unnecessary levels of complexity). \r\nRefactoring is 1 of the primary means of repaying technical debt\r\n- Error message: TypeError: unhashable type :'DataFrame' normally happens when we try to use a pandas DataFrame (which is\r\nmutable, hence unhashable) +. using DataFrame as a dictionary ky or in a set, inccorect usage of .loc or iloc, passing DataFrame to a function\r\nexpecting a hashable type\r\n\r\n*** Things I did well ***\r\n- Figuring out a way to write a function to perform some analysis on multiple dataframes \r\n\r\n*** Things I did not do well ***\r\n- Wrote a function to perform some analysis on multiple dataframes that did not work\r\n\r\n*** Things I will do better next time ***\r\n- Ask Bing for some idea\r\n- Understand why it did not work \r\n\r\n9/1/2023\r\n***Things I learned today:***\r\n- zip (): a built-in function taking 2 or more iterables and returns an iterator \r\naggregating elements from each iterable. The resulting iterator contains tuples, where\r\nthe i-th tuple contains the i-th element from each of the input iterables. \r\n- plotly.graph_objs: a module in plotly providing a set of classes for creating and manipulating figures. \r\nWe call these \"graph objects\" and represent parts of a figure. \r\n- No def needed when creating a plot. Simply no need to. \r\n- Plotly is a data visualisation library allowing you to create interactive plots and \r\ncharts in Python. Advantages: interactive visualisations, ease of use, customisation, compatibility,\r\ncommunity support. In general, if your data set is small and simple, and you only need to communicate a few key insights, a static visualization might be the best choice. \r\nHowever, if your data set is large and complex, and you need to allow the user to explore the data in more detail, an interactive visualization might be a better choice.\r\n- A Scatter object: used to create line charts, scatter charts, text charts and bubble charts. \r\n***Things I did well today:***\r\n- Understanding how to write a function to take 2 or more iterables and return a iterator aggregating\r\nelements from each iterable using zip. The function takes 2 lists from stock data and stock names to print information, shape, data types and statistical descriptions. \r\n- Understanding how to write a function to reset index of DateTimeIndex to a column of stock data using enumerate. \r\n- Learning to use Plotly to create an interactive plot since it has advantages of interactive\r\nvisualisations, ease of use, customisation, compatibility and community support. When the dataset is complex and large, we can use it.\r\n\r\n***Things I didn't do well today:***\r\n- When visualising stock data Price vs. Time, there were multiple plots created, not just 1 plot needed.\r\n- Didn't write the code completely off my head\r\n\r\n*** Things I will do better next time: ***\r\n- Chunking code in memory and recreate it \r\n- Looking into why go.Scatter created multiple plots instead of one \r\n\r\n12/1/2024\r\n***Things I learned today***\r\n- Building a Dash application requires 2 parts: the first one is the layout of the app, describing what the app looks like, \r\nthe 2nd part describe the interactivity of the app\r\n***Things I did well today ***\r\n- Reconstructing the code to reset all of DateTimeIndex for all stock data from memory\r\n- Learning how to build a Dash app\r\n***Things I did not do well today ***\r\n- Unable to build the Dash app using dcc.Graph (deprecated)\r\n***Things I will do better next time ***\r\n- Figure out how to build a Dash app using dcc.Graph, perhaps checking with Amin or JetBrain support team \r\n\r\n15/01/2024\r\n***Things I learned today ***\r\n- In Dash @callback, 'my-id' and 'value' correspond to the unique identifier and current value of the component, which needing lookup\r\n- In @callback, 'inputs' and 'outputs' are arguments of callback decorator. Whenever an input changes, the function the callback decorator wraps around will change automatically\r\n- To detect outliers in stock data, we can use the method of Interquartile Range (IQR). It is the range between the first quartile (25 percentile) and the second quartile (75 percentile).\r\nAny data points fall below the Q1 - 1.5*IQR and above the Q2 + 1.5*IQR will be outliers.\r\n***Things I did well today***\r\n- Figuring out how to build an interactive Dash visualisation app\r\n- Figuring out how to conduct Exploratory Data Analysis\r\n- Detecting outliers using IQR method\r\n\r\n*** Things I did not do well today ***\r\n- Not getting the dash app to work\r\n- Taking too much time with the dash app\r\n\r\n*** Things I will do better next time ***\r\n- Only spend 1 hour on Dash app. If not working, make notes to ask\r\n- Spend more time on data analysis \r\n\r\n19/01/2024\r\n***Things I learned today***\r\n- if plt.show() outside the loop, only show the box plot last iteration (pypl). If inside, show all the box plots\r\n- Reasons for outliers in stock market data: market volatility, significant news or events, earnings reports, changes in \r\nmarket sentiments, and data entry errors\r\n- Outliers, though often viewed as 'data problems' we need to fix, may offer interesting explanations that can lead to \r\ntheoretical insights since we can study them as unique phenomena. Therefore, we need to develop clear guidelines and a better understanding\r\nin 3 areas: a) how to define them, b) how to detect them, c) how to handle them. \r\n\r\n***Things I did well today***\r\n- Writing clearer, concise and higher quality code to replace the old one\r\n- Getting better at understanding the stock market data\r\n- Looking into outliers, reasons for their existence and how to handle them\r\n\r\n***Things I did not do well today***\r\n- Not understanding how to handle outliers in stock market data yet\r\n- Talking to Hayden while doing deep work\r\n\r\n***Things I will do better next time***\r\n- Asking Hayden to go out for a walk while doing deep work or simply moving to the library\r\n- Better grasp how to handle outliers in stock market data\r\n\r\n\r\n22/01/2024\r\n*** Things I learned today ***\r\n- Considerations about outliers: outliers can lead to important changes in parameter estimates when researchers \r\nuse statistical methods relying on maximum likelihood estimator and how we deal with outliers\r\nmay lead us to falsely accept or reject hypotheses; we must use caution since in most cases, deleting \r\noutliers helps us support our hypotheses and opting a course of action post hoc that is certain to increase\r\nour likelihood of finding what we want to find is a dangerous practice.\r\n- The decisions researchers make about how to define, identify, and handle outliers have\r\ncrucial implications, because such decisions change substantive conclusions.\r\n- 14 outlier definitions: single construct outlier, error outlier, interesting outlier, discrepancy outlier,\r\nmodel fit outlier, prediction outlier, influential meta-analysis effect size outlier, influential meta-analysis\r\nsample size outliers, influential meta-analysis effect and sample size outlier, cluster analysis outlier, influential\r\ntime series additive outlier, influential series innovation outliers, influential level shift outlier,\r\ninfluential temporary changes outlier\r\n\r\n***Things I did well today ***\r\n- Reading and scanning papers about outliers\r\n- Digging deeper into outliers\r\n\r\n*** Things I did not do well today ***\r\n- Taking more time to read. Should take about 3 - 4 hours to read\r\n\r\n*** Things I will do better next time ***\r\n- Speeding up my reading, perhaps can skim some information\r\n\r\n24/01/2024\r\n***Things I learned today***\r\n- outlier identification techniques: box plot, stem and leaf plot, schematic plot analysis,\r\nstandard deviation analysis, percentage analysis, scatter plot, q-q plot, p-p plot, standardised residual,\r\nstudentised deleted residual, euclidean distance, mahalanobis distance, K-clustering,\r\n2- or 30 dimesional plots of the original and PCA, autocorrection function plot, time plot, extreme studentised deviate,\r\nHosmer and Lemeshow goodness-of-fit test, Leverage value, centred leveraged value, deletion standardised multivariate residual,\r\nCook's Dt, Modified Cook's Dt, Generalised Cook's Dt,difference in fits (standardised), \r\ndifference in beta (standardised), Chi-squared difference test, single parameter influence, average squared \r\ndeviation technique, sample-adjusted meta-analytic deviancy, conduct anaysis with and without outliers, nearest neighbor techniques,\r\nnonparametric methods, semiparametric methods, iterative outlier identification procedure, independent component analysis\r\n\r\n***Things I did well today***\r\n- Understanding outlier detection techniques in detail\r\n\r\n***Things I did not do well today***\r\n- Slow speed in reading the paper\r\n\r\n***Things I will do better next time***\r\n- Using abbrev, shortening sentences, copying \r\n\r\n***Things I learned today***\r\n- Standardised residual: calculated by dividing the ith observation's residual value by a std deviation term\r\n- Studentised residual: measuring both the outlyingness of the observation regarding its standardised residual\r\nvalue and the outlyingness of the observation on the predicted value. \r\n- Standardised deleted residual: the predicted value for the focal observation is calculated without the observation itself\r\n- Euclidean distance: length of the line segment between 2 specified points in different dimension space, can be calculated\r\nfrom the Cartesian coords of the points using the Pythagorean theorem. \r\n- Mahalanobis distance: similar to Euclidean distance, the length of the line segment between a data point and the centroid\r\nof the remaining cases, where the centroid is the point created at the intersection of the means of all the prediction vars.\r\nA large Mahalanobis distance may mean the corresponding observation is an outlier. \r\n- K-clustering: yield different candidate subsets that then have to be evaluated by 1 or more multiple-case diagnostics\r\n- 2 - or 3-dimensional plots of the original and the principal component vars: produced as a result of a principal \r\ncomponent analysis. A isolated data point may indicate a potential outlier.\r\n- autocorrelation plot: computing autocorrelations for data valyes at varying time lags. \r\n- time plot: relationship between a certain variable and time.\r\n- extreme studentised deviate: different btwn a var's mean and query value, / a std value\r\n- Hosmer and Lemeshow goodness-of-fit test: a Pearson chi-square stat from a table of observed and expected freq.\r\n\r\n***29/01/2024***\r\n***Things I did well today***\r\n- Identifying outliers and factors might cause them\r\n- Looking at different methods and techniques to detect outliers\r\n\r\n***Things I did not do well today***\r\n- Taking some time to read the paper\r\n\r\n*** Things I will do better next time***\r\n- Give myself more time to read\r\n\r\n***31/01/2024***\r\n\r\n***Things I did well today***\r\n- To calculate regression coefficients:\r\n1. Fit a linear regression model to our data.\r\n2. Calculate the regression coefficients using:\r\nThe coefficient of X is given by the formula a = n(∑xy)−(∑x)(∑y) / n(∑x2)−(∑x)2\r\nThe constant term is given by the formula b = [(∑y)(∑x^2)−(∑x)(∑xy)] / [n(∑x2)−(∑x)2]\r\n3. Lastly, we can input the regression coefficients in the equation Y = aX + b to obtain\r\nthe predicted vals of Y. \r\n- Cook's Di: assessing the influence a data point i has on all regression coefficients as a whole, \r\ncalculated by deleting the observation from the dataset and then recalculating the regression coefficients. \r\nDi = [(ri^2)/(p*MSE)]/[(hii)/(1-hii)^2], where ri denotes the ith residual, p is the number of coefficients in the \r\nregression model, MSE is the mean sqrd error, hii is the ith leverage value\r\n- Modified Cook's Di: using standardised deleted residuals rather than standardised residuals\r\nDi* = (Di)/(p*MSE) * (hii)(1-hii)^2, where Di: Cook's distance, p is the number of coefficients in the regression model,\r\nMSE is the mean squared error, hii is the ith leverage val.\r\nTaking into acc the number of independent vars in the model. \r\n- Generalised Cook's Di: applied to structural equation modelling to assess the influence a data point has on\r\nthe parameter estimates.\r\nDi^* = ri^2/(p * MSE *hii), where rii denotes the ith residual, p is the number of coeffs in the regression model,\r\nMSE is the mean squared error, hii is the ith leverage val.\r\n- Structural equation modelling: method used to analyse the relationships between the observed and latent vars. Combining\r\nfactor analysis and multiple regression analysis. We create a model representing how various aspects of some phenomenon\r\nthought to causally connect to one another. Often contain postulated causal connections among some latent vars. Additional\r\ncausal connections link those latent vars to observed vars. \r\n- latent vars: vars we cannot directly observe or measure, but inferred from other observable vars that can be directly observed\r\nor measured via a mathematical model, representing underlying concepts or construct we cannot directly measure.  \r\n- Difference in fits, standardised (DFFITSi): assessing the influence a data point i has on all regression coeffs as a whole,\r\nDiff between Cook's Di and this is that they produce info on different scales. A diagnostic measure to detect influential \r\nobservations, measuring the diff in the predicted val for a point, obtained when that point is left out of the regression.\r\nDFFITS is the Studentised DFFIT where we can achieve studentisation by dividing the estimated std of the fit at that point.\r\nDFFITSi = (^yi - ^y(i))/(sqrt(MSEi x hii)), where ^yi denotes the predicted val for the ith observation, \r\n^y(i) the predicted val for the ith obs when we exclude it from the regression, MSE(i) the mean sqrd error when the \r\nith obs is excluded from the regression, hii the ith leverage value \r\nObs with DFFITS val greater than 2x the sqr root of the no of parameters / the no of obs are considered to be influential \r\n\r\n- Difference in beta, standardised (DFBETASij): assessing if the inclusion of a case i leads to an increase or decrease in a single regression coeff\r\nj i.e. a slope or intercept. Measuring the difference in the estimated regression coeff for a predictor car when that obs is included or excluded \r\nfrom the regression model. \r\nDFBETAS(ij) = [beta_i - beta(i(-j)]/[sqrt(MSE(-i) x hii))]\r\n\r\n- Chi-squared difference test: allowing a researcher conducting SEM to assess the diff in the model\r\nfit between 2 models, 1 with the outlier and the other without \r\nTo conduct a chi-squared difference test:\r\n1. Define a null hypo and the alternative hypo\r\n2. Calculate the expected frequencies for each category under the null hypo\r\n3. Need to calculate the chi-squared test stat using this:\r\nchi^2 = sum{i=1}^{k}\\frac(O_i = E_i)^2}{E_i}\r\nwhere:\r\nO_i is the observed frequency for the category i\r\nE_i is the expected frequency for category i\r\nk is the number of categories\r\n4. Need to compare the calculated chi-squared test stat to the critical value of the \r\nchi-squared distribution with k-1 degrees of freedom. If the test sta > \r\nthe critical val => reject the null hypothesis to conclude that there is a sig diff \r\nbetween the observed and expected frequencies of the categorical var.  \r\n- Single parameter influence: used in SEM to assess the effect of an outlier on a specific parameter \r\nestimate, as opposed to the overall influence of an outlier on all para estimates\r\nIdentifying which parameters have the greatest impact on the model's output. We can calculate\r\nusing various methods: DIFFITS, DFBETAS, and Cook's distance. \r\nA common rule of thumb: observations with a DIFFITS val greater than 2 x the sqr ropt of the no of \r\nparameters/the no of obs considered to be influential. Also, points with a large Cook's distance are considered\r\ninfluential. \r\n- Avg squared deviation technique: when conducting multilevel modeling, explores the effect each group\r\nhas on the fixed &/or random parameters, allowing for the identification of higher-level prediction outliers. \r\nMeasuring the variability of a set of data points. \r\nCalculated by finding the diff between each data point & the mean if the data set, squaring the differences, adding \r\nthem together, and dividing by the no of data pts - 1. \r\ns^2 = (sum_{i_1}^{n}({x_i - \\bar{x})^2/{n-1}\r\nwhere:\r\ns^2: the avg sqrd deviation \r\ns_i is the ith data pt\r\nx_i: the ith data pt\r\n\\bar{x} is the mean of the data set\r\nn : the no of data pts\r\n\r\nA large avg sqrd deviation: indicates the data pts are more spread out, while a small avg sqrd deviation conveys data points are tightly clustered around \r\nthe mean. \r\n\r\n\r\n- ***Things I did well today***\r\n- Reading in-depth about outlier identification techniques\r\n\r\n***Things I did not do well today***\r\n- Woke up a bit late to study\r\n***Things I will do better next time***\r\n- Have an alarm ready\r\n\r\n***05/02/2024***\r\n***Things I learned today***\r\n- Sample-adjusted meta-analytic deviancy (SAMD): in metaanalysis, it takes the diff between the val of each primary-level\r\neffect suze estunate and the mean sample-weighted coeff  computed without that effect size in the analysis, then alternates \r\nthe diff val based in the sample size of the primary-level study. Can use SAMD to detect outliers. Helps us to identify\r\nstudies whose effect sizes deviates significantly from the overall trend. Calculates external studentised residuals for each study.\r\nBy considering the sample size, SAMD gives us a more robust way to detect outliers and helps researchers identify studies that might be driving \r\nunusual patterns in the meta-analysis. \r\n- Conduct analysis with or without outliers: when results differ across the 2, we confirm that the data pt is indeed an outlier. Why:\r\nsensitivity check, transparency, understanding impact.\r\nHow: document, describe approach, report the diffs\r\n- Nearest Neighbor techniques: calculate the closest val to the query val using various types of distance metrics. \r\nTechniques: KNN, optimised NN, 1-NN, 2-NN, NN with reduced features, dragon method, PAM (partitioning around medoids),\r\nCLARANS (clustering large apps based on randomised search) and graph connectivity method.\r\n\r\n- Nonparametric methods: fitting a smoothed curved without making any constraining assumptions about the data. Lack of \r\nlinear relationships indicates the presence of outliers. Not relying on specific assumptions about the parameters of the\r\ndata distribution. Often used when the assumptions of the parametric methods are violated. \r\nCharateristics: distribution-free, function on samples (defining stats as functions on samples dependency on specific parameters). \r\nCan use them for descriptive stats, statistical inference, modelling financial time series. \r\n- Semiparametric methods: combining the speed and complexity of parametric methods with the flexibility of nonparametric methods\r\nto understand local clusters or kernels rather than a single global distribution model. We identify outliers as lying in regions of \r\nlow density. Striking the balance of flexibility and structures.  \r\nComponents of semiparametric models: \r\nparametric components (representing a finite-dimensional vector) & nonparametric components (representing a infinite-dimensional vector)\r\nAdvantages: flexibility, robustness, interpretability/ \r\nChallenges: estimation, curse of dimensionality\r\nApplicationL biostats, econs, env sciences\r\n- Iterative outlier identification procedure: allowing for the estimate of the residual std to identify\r\ndata pts sensitive to the estimation procedure used for a time series analysis. \r\nHow:\r\nIdentify potential outliers\r\nRemove or adjust outliers\r\nRecompute stats\r\nRepeat step 1-3\r\n- Independent component analysis: separate independent component by maximising the \r\nstatistical independence among them. We identify the separate independent components \r\nas outliers. \r\nObjective: separate a multivariate signal into additive subcomponents, \r\nassuming at most subcomponent is Gaussian (normal), and the subcomponents are statistically independent\r\nfrom each other. \r\nGoal: unmix and decompose signal into its original independent sources\r\nKey assumptions: source signals are independent of each other. Values in each source signal\r\nhave non-Gaussian distribution\r\n\r\nApplication: cocktail party problem (separate overlapping speech signals from \r\nmultiple speakers), blind source separation (extracting individual sources from their\r\nmixture), biomedical signal processing (separating EEG signals, fMRI data), image processing\r\n(texture analysis, face recognition). financial data (separating market signals)\r\nMathematical approach: attempt to find a linear transformation of the data such that \r\nthe xformed components are independent as possible. works well when the statistical \r\nindependence assumption holds.\r\nICA goes beyond PCA by seeking statistically independent components\r\n\r\nA good way to disentangle complex mixture and reveal hidden structures in data\r\n\r\n***Things I did well today***\r\n- Understanding outlier detection methods more in-depth\r\n- Be more on-time \r\n- More focus\r\n- Identifying the Iterative outlier identification procedure as a suitable procedure for this project\r\n***Things I did not do well today***\r\n- Syntax error with using ' ' instead of \" \" after f\r\n- Not completing the analysis\r\n\r\n***Things I will do better next time***\r\n- using f \" \"\r\n- figuring out more ways to conduct analysis to make it more complete\r\n\r\n\r\n*** 7/2/2024***\r\n***Things I learned today***\r\n- Correct val: correcting a data pt to its proper val.\r\n- Remove outlier: eliminate the data pt from the analysis.\r\n- Study the outlier in detail: conduct follow-up work to study the case\r\nas a unique phenomenon of interest. Analyse outlier, study its impact\r\n- Keep: acknowledge the presence of an outlier, but do nothing prior to the\r\nanalysis\r\n- Report findings with and without outliers: report substantive results with \r\nand without, include any explanation for any diff in the results. Be transparent.\r\n- Winsorisation: transforming extreme vals to a specified percentile of the data\r\n- Truncation: setting observed vals within a certain range, anything outside we will\r\nremove\r\n- Transformation: applying a deterministic mathematical function to each val.\r\n- Modification: changing an outlier to another val, less extreme one.\r\n- Least absolute deviation: similar to ordinary least squares (method to estimate the \r\nunknown parameters in regression). Better than LSD when errors follow non-Gaussian distribution with longer tails. \r\n- Least trimmed squares: ordering the squared residual for each case from the highest to the lowest,\r\nthen trim or remove the highest val. \r\n- M-estimation: a class of robust techniques reducing the effect of influential outliers\r\nby replacing the squared residuals by another func of them. \r\n- Bayesian stats: obtaining parameter estimates by weighing prior info and the observed data at hand\r\n- 2-stage robust procedure: use Mahalanobis distance to assign weights to each data pt, extreme cases\r\nare downweighted. completed via a 2-stage process.\r\n\r\n***Things I did well today***\r\n- Learning more about how to handle outliers: correct val, remove outlier. study the outlier in detail,\r\nkeep, report findings with or without the outlier, winsorisation, truncation, transformation, modification, \r\nleast absolute deviation, least trimmed squares, M-estimation, Bayesian stats, 2-stage robust procedure\r\n\r\n***Things I did not do well today***\r\n- Not writing any code\r\n\r\n***Things I will do better next time***\r\n- Writing some code: will do some PCA maybe? \r\n\r\n***12/02/2024***\r\n***Things I learned today***\r\n- Direct robust method using iteratively reweighted least squares: using Mahalanobis distance to\r\nassign weights to each data pt. We complete the assignment of weights via using an iteratively reweighted least \r\nsquares algo. IRLS also commonly used for robust regression, especially when dealing with outliers or heavy-tailed \r\nerror distributions. IRLS aim to mitigate the impact of outliers by downweighting their influence during parameter \r\nestimation. IRLS updates the regression coeffs by reweighting the observations based on their residuals: initialisation, \r\nweight calculation, re-estimation, converge check, final estimates. \r\n- generalised estimating equations (GEE) methods: estimating the variances and covariances in the random part of the \r\nmultilevel model directly from the residuals. Useful for analysing correlated data, allowing to acc for dependencies and estimate pop-up effects\r\n- Multilevel models: statistical techniques used to analyse data with a hierarchical or nest structure. Level 1 model: describe indv-level relationships. \r\nLevel-2 model: describe group-level relationships\r\n- Boostrapping methods: estimate parameters of a model and their standard errors from the sample, without reference to a theoretical sampling distribution. \r\nApp: statistical inference, regression models, ML\r\n- Non-parametric methods: does not assume the data distributed in any particular way\r\n- Unweighted meta-analysis: obtaining meta-analytic stats not giving more weight to primary level studies with large sample sizes. How: each \r\nstudy contributes equally to the overall effect estimate.\r\n- Generalised M-estimation: a class of robust techniques reducing the effect of outliers by replacing the squared residuals by another func of the residuals.\r\n- 3 shortcomings authors identify: 1st => it is common for organisational science researchers to either vague or not transparent on how outliers are defined\r\nand how a particular outlier identification method chosen and used, 2nd => we identify outliers in one way but then used another outlier identification technique not \r\ncongruent with their adopted outlier definition, 3rd => the authors found little discussion, let alone recommendations, on the subject of studying outliers interesting \r\nand worthy of further examination. \r\n- A pervasive view: outliers are problems that we should fix\r\n- 2 principles: we should describe choices and procedures regarding the treatment outliers that we have implemented, and we should clearly and explicitly acknowledge the type\r\nof outlier where they are interested and use an identification technique congruent with the outlier definition\r\n\r\n***Things I did well today***\r\n- Learning about: direct robust method using iteratively reweighted least squares, generalising estimating equations methods, boostrapping methods, non-parametric method, unweighted \r\nmeta-analysis, generalised M-estimation, 3 shortcomings, 2 principles\r\n***Things I did not do well today***\r\n- Not defining an outlier yet***\r\n*** Things I will do better next time***\r\n- Defining outliers\r\n\r\n***13/02/2024***\r\n***Things I learned today***\r\n- 3 types of outliers: error outliers, interesting outliers and influential outliers\r\n- 2 steps of detecting error outliers: 1st step => locating outlying obs, 2nd => separately \r\ninvestigating whether the outlyingness of such data pts was caused by errors\r\n- 2 categories of techniques to identify error outliers: single construct and multiple construct\r\n\r\n***Things I did well today***\r\n- Learned about types of outliers, steps of detecting error outliers and 2 categories to \r\nidentify error outliers\r\n\r\n***Things I did not do well today***\r\n- Did not study for the whole 3 hours due to tiredness and tummyache\r\n\r\n***Things I will do better next time***\r\n- Anticipate beforehand to fix my schedule\r\n\r\n***16/02/2024***\r\n***Things I did well today***\r\n- Single construct techniques: refer to the measurement of constructs expected\r\nto have a single underlying dimension. 2 steps: conceptualisation and operationalisation. \r\nThe recommendation is using visual tools first then follow up with quantitative approaches. Can use \r\nrecommended cutoffs. \r\n- Multiple construct techniques: use them when we believe a concept or construct\r\nto have multiple dimensions, using multiple measures or tests to capture the different\r\ndimensions of a construct. 2 steps are similar to single construct techniques. Use\r\nrecommended cutoffs when applicable. \r\n- Construct: abstract concepts not directly observable\r\n- Researchers should keep diaries, logs or journals during data collection to use \r\nretrospectively to decide if something unusual happened with some particular case that \r\nthey can no longer trace after the fact. \r\n- The 2nd step in the process of understanding the possible presence of outliers is \r\nexamining interesting outliers. Do not automatically treat any outlying data pts as harmful. \r\n- Interesting outliers: accurate data pts, identified as outlying obs ( potential error outliers)\r\nbut not confirmed as actual error outliers. These cases may contain potentially valuable or unexpected knowledge\r\n- Identifying interesting outliers involve 2 steps: identifying potentially interesting outliers and\r\nidentifying which outliers are indeed interesting. \r\n- Pursuing potential interesting outliers likely will include the examination of \r\na great deal of error outliers going undetected as errors.\r\n\r\n***Things I did well today***\r\n- Focusing on my study of detecting and handling error outliers and interesting outliers\r\n\r\n***Things I did not do well today***\r\n- Talking to Hayden during deep work\r\n\r\n***Things I will do better next time***\r\n- Informing Hayden about my deep work session\r\n\r\n***20/02/2024***\r\n***Things I learned today***\r\n- We address influential outliers differently from error outliers and interesting outliers depending \r\non particular statistical techniques. \r\n- 2 types of influential outliers: model fit (presence changes the fit of the model) and prediction (change the parameter estimates of the model).\r\n- 2 step process to identify model fit outliers: identifying data pts most likely to have influence\r\non the fit of the model since they deviate markedly from other cases in the dataset, investigating \r\nthese cases to understand if they genuinely have influence on the model fit. \r\n- 3 techniques to assess the presence of pred outliers in regression: DIFFITS, DFBETAS, Cook's Di\r\n- 3 courses of action to handle model fit and pred outliers: respecification (adding other terms to the regression equation,]\r\ndeletion and robust approaches (involving non-OLS standard). We should always report results with \r\nand without the technique.\r\n- the process of identifying model fit outliers in SEM is similar to regression\r\n- In regression, there are 2 types of pred outliers: global (impact all parameter estimates of the model) \r\nand specific (impacting 1 parameter estimate)\r\n- We should use gCDi stat to detect global pred outliers while not neglecting the detection\r\nof specific pred outliers (using the standardised change in the jth parameter resulting \r\nfrom the deletion of the obs i)\r\n- Handling influential outliers in SEM is similar to in regression. Recommended the use of deletion\r\nor robust approaches\r\n- In multilevel modelling, the main goal of an analysis is assessing the size of the variance\r\ncomponents and the sources of such variances. \r\n- The recommendation is a top-down approach in identifying model fit outliers in multilevel modelling,\r\nbeginning at the highest level of analysis. The researcher then can decide whether a group of obs\r\naffects the model fit b/c of a) the group itself and/or b) a particular data pt(s) in the group\r\n- In multilevel modelling, we use a top-down approach. The recommendation is calculating \r\nthe avg sqrd deviation, then the researcher can compare Cj vals against one another using an index plot. \r\n\r\n***Things I did well today***\r\n- completing correlation heat maps\r\n- Learning about: influential outliers and how to handle them in regression, SEM and multilevel modelling\r\n\r\n***Things I did not do well today***\r\n- Not finishing the paper on identifying and handling outliers\r\n\r\n***Things I will do better next time***\r\n- Finishing the paper\r\n\r\n***22/2/2024***\r\n***Things I learned today***\r\n- Handling influential outliers in multilevel modeling are similar to those used in regression and SEM. But, unlike regression,\r\nresearchers need to 1st decide the level where any additional predictor(s) are to be added in the multilevel modelling context\r\n- Options for robust techniques in multilevel modeling include: generalised estimating equations (GEE) and bootstrapping methods\r\n\r\n***Things I did well today***\r\n- Finishing the paper\r\n- Code to visualise Q-Q plots \r\n\r\n***Things I did not do well today***\r\n- Not reading other papers about outliers \r\n- Not doing time series analysis\r\n\r\n***Things I will do better next time***\r\n- Reading more papers about outliers\r\n- Doing time series analysis\r\n\r\n***27/02/2024***\r\n***Things I learned today***\r\n- ValueError: an exception raised when a function receives an argument of the correct type but an inappropriate value. \r\n- AttributeError: an exception raised when an attribute reference or assignment fails, normally occurs when\r\ntrying to access or modify an attribute or method that doesn't exist for a specific object or class \r\n- In Python, flattening an array means transforming a multi-dimensional array to a one-dimensional array to make it \r\neasier to iterate over\r\n- We can ask Copilot to make our code more concise\r\n\r\n***Things I did well today***\r\n- Coding Q-Q Plot\r\n- Coding distribution tests\r\n\r\n***Things I did not do well today***\r\n- Not being able to code box plots \r\n\r\n***Things I will do better next time***\r\n- Coding box plots for all columns\r\n\r\n***29/02/2024***\r\n***Things I learned today***\r\n- https://www.machinelearningplus.com/plots/matplotlib-plotting-tutorial/\r\n- KeyError: occuring when trying to access a key that isn't in a dictionary or dictionary-like object\r\n- TypeError: occuring when types don't match when performing Python operations\r\n- to draw a line plot using matplotlib, ordinals have to meet the requirements\r\n\r\n***Things I did well today***\r\n- Trying to use matplotlib to draw line plots\r\n- Using plotly to draw line plots \r\n\r\n***Things I did not do well today***\r\n- Encountering problems with matplotlib\r\n\r\n***Things I will do better next time***\r\n- Fixing matplotlib\r\n\r\n***5/3/2024***\r\n***Things I learned today***\r\n- If the error \"Permission denied\" => no access to Python packaging tools => need to add the variable to PATH. Also need to create a different\r\nvenv to test\r\n- decomposition models can be additive or multiplicative. Additive: when seasonality and irregular variations don't change as much as trends changes.\r\nMultiplicative: when seasonality and irregular variations increase in amplitude as trend changes\r\n- When different features have different scales: consider adding another y-axis or normalise the data\r\n\r\n***Things I did well today***\r\n- Fixing matplotlib\r\n\r\n***Things I did not do well today***\r\n- Not finishing with visualising all features, including Volume with plotly\r\n- Not finishing with seasonal.decompose yet \r\n***Things I will do better next time***\r\n- Visualising all features with a line plot\r\n- Finishing with seasonal.decompose\r\n\r\n***7/3/2024***\r\n***Things I learned today***\r\n- Multivariate normality: when a dataset has multiple variables and these variables have data points\r\nthat are normally-distributed together. To assess whether multivariate normality exist: visual inspection (scatter plot\r\nfor pair vars to see if there are signs of elliptical patterns), PCA (checking variance), Mardias's test\r\n- PCA: identify trends and relationships, help with dimensionality reduction and feature extraction\r\n\r\n***Things I did well today***\r\n- Drawing scatter plots for variables to see their relationships\r\n- Checking for multivariate normality \r\n- Performing PCA\r\n\r\n***Things I did not do well today***\r\n- Spending a lot of time with Shapiro-Wilk test programming with stock data when not knowing whether it is right for them\r\n- Not finishing multivariate normality check\r\n\r\n***Things I will do better next time***\r\n- Checking the business problem at hand\r\n- Finishing multivariate normality check and understanding why multivariate normality\r\n\r\n***14/03/2024***\r\n***Things I learned today***\r\n- explained variance: the proportion of total variability accounted for by a component/factor. The higher, the more influence it has\r\non the data \r\n- loadings: weights of components, indicating the covariance/relationships between original features and PCA-scaled units\r\n- to understand PCA: check explained variance and loadings, relating these back to specific domain knowledge. To engineer new features, the \r\nhigher explained variance the better\r\n\r\n***Things I did well today***\r\n- finishing code for PCA\r\n\r\n***Things I did not do well today***\r\n- Not saving what I found using PCA\r\n- Not grasping why multivariate normality\r\n\r\n***Things I will do better next time***\r\n- Continuing with PCA\r\n- Understanding why multivariate normality\r\n\r\n***18/03/2024***\r\n***Things I learned today***\r\n- programming to print out relationships and patterns in PCA \r\n- Rule of thumb: if loading >0.3 => substantial relationship with feature. \r\n- df.loc [row,col] => locating the actual item. df.index = row, df.columns = col\r\n- Why multivariate normality: regression analysis, PCA, ML and algorithms, factor analysis, hotelling's t-squared test (comparing means of multivariate data),\r\nquality control and process monitoring, portfolio theory and risk management, robustness and sensitivity, graphic representations\r\n***Things I did well today:***\r\n- Coding to print out relationships between explained variance and the dataset, loadings and feature\r\n- Getting an understanding why multivariate \r\n***Things I did not do well today:***\r\n- wrong for loop for loadings, why print out the same results 3 times?\r\n\r\n***Things I will do better next time:***\r\n- Check my understand of how computer thinks and for loop\r\n\r\n***20/03/2024***\r\n***Things I learned today***\r\n- seasonal decomposition => additive (trend and irregularities remain stable as time increases),\r\nmultiplicative (trend and irregularities change as time increases)\r\n- high cumulative variance: variables explain the data, components retain most of the info in the original variables. low cumulative variance: the data varies sporadically \r\n=> variables cannot explain the data, total variance spread out across many components\r\n\r\n***Things I did well today***\r\n- Digging deeper into PCA\r\n- Starting seasonal decomposition\r\n***Things I did not do well***\r\n- Taking some time to conduct PCA\r\n- Not looking at how computers think and for loop\r\n***Things I will do better next time***\r\n- Checking what I will do better first \r\n- Spending less time on PCA\r\n\r\n***22/3/2024***\r\n***Things I learned today***\r\n- To test for multivariate normality, PCA is not a reliable measure. We need to employ Mardia's test or Henze-Zirkler's test.\r\n- Henze-Zirkler's test is based on a nonnegative functional distance measuring the distance between 2 distributions. Key\r\npoints: test statistic (approximately distributed as lognormal), distance measure(measuring the distance between the characteristic function of a multivariate normality\r\nand the empirical characteristic distribution), consistency: Henze-Zirkler is a consistent test, as the sample size increases, the power of Henze-Zirkler to test\r\nif the null hypothesis is false approaches 1. \r\n- Mardia's test: based on 1st and 3rd moments of the data distribution, (skewness and kurtosis) => based on chi-squared distribution (widely used for inferential statistics).\r\nSkewness test and kurtosis test \r\n\r\n***Things I did well today***\r\n- Conducting Henze-Zirkler's test and Mardia's test for multivariate normality\r\n- Fixing up the code for CPA\r\n\r\n***Things I did not do well today***\r\n- PCA code is not concise enough\r\n- Not fully grasping Mardia's test and chi-squared distribution\r\n- Not conducting analysis for seasonal decomposition \r\n***Things I will do better next time***\r\n- Shortening PCA code\r\n- Grasping Mardia's test, Henze-Zirkler's test and chi-squared distribution\r\n\r\n***Things I learned today***\r\n- Chi-squared distribution: calculated by the sum of the squared of normal random variables, the shape is determined by \r\ndegree of freedom. Used on various statistical tests.\r\n- Period of seasonal decomposition depends. For stock data, could be yearly(1) or quarterly(4)\r\n- Mardia's test: calculate \r\n- Henze-Zirkler's test\r\n\r\n***Things I did well today***\r\n- Finishing seasonal decomposition and analysis of it\r\n- Finishing correlation heatmaps\r\n\r\n***Things I did not do well today***\r\n- Not conducting other testing for outliers\r\n- Not finishing up CPA without outliers\r\n\r\n***Things I will do better next time***\r\n- Conducting other tests for outliers\r\n- Finishing up CPA without outliers\r\n\r\n***Things I learned today***\r\n- Mardia's test: measuring the distance between x_i and x_j. The inverse of covariance matrix is the precision matrix (keeping the other variable constant \r\nwhile calculating the relationships between 2 other variables)\r\n- Adjusted Close: the price of Close, but changed due to corporate actions\r\n\r\n***Things I did well today***\r\n- Learning about the maths behind Mardia's test in-depth. \r\n- Calculating the correlation coefficients and analyse correlations between features\r\n\r\n***Things I did not do well today***\r\n- Not finishing CPA without outliers\r\n- Not conducting other tests for outliers yet\r\n\r\n30/3/2024\r\n***Things I learned today***\r\n- Calculating mahalanobis distance\r\n***Things I did well today***\r\n- Writing the function to calculate mahalanobis distance\r\n***Things I did not well today***\r\n- Encountering ValueError while calculating mahalanobis distance\r\n***Things I did well today***\r\n- Fixing the error\r\n\r\n4/1/2024\r\n***Things I learned today***\r\n- Bonferroni correction: fixing the alpha level for multiple tests since it reduces type I errors \r\n- chi-squared distribution: a special case of gamma distribution, used for hypothesis testing. critical value based on alpha level and degree of freedom (no of vars -1 )\r\n- F-distribution: for 2 chi-squared distribution.Critical value based on alpha level & degree of freedom for numerator and denominator \r\n- The data has multicollinearity, when calculating sigma, need to add a jitter (ridge regression) to have an inverse since the sigma is a singular matrix \r\nwith no inverse (why?)\r\n***Things I did well today***\r\n- Finishing calculating mahalanobis\r\n- Understanding more about data distribution\r\n\r\n***Things I did not do well today***\r\n- Not fully understanding why a singular matrix doesn't have a inverse\r\n- No PCA without outliers\r\n***Things I will do better next time***\r\n- Understanding more about inverse matrix\r\n- PCA without outliers\r\n\r\n***3/4/2024***\r\n***Things I learned today***\r\n- https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/full/10.1002/cem.2692\r\n- self: reference to the class\r\n- __init__: initialise a class\r\n- class: dictate behaviors of objects \r\n  - Inverse matrix: a matrix when multiplied with our matrix will produce a matrix with 1 along the diagonal. To calculate an inverse matrix\r\n        - A determinant: calculate the determinant of the matrix\r\n        - An adjucate matrix: find cofactors (sign factor multiplied by minor) and replace all the elements of the original matrix with the corresponding cofactors\r\n        - An inverse matrix: 1/det(A) * adj(A)\r\n***Things I did well today***\r\n- Writing class for better and more maintainable code \r\n- Understanding more about inverse matrices\r\n***Things I did not do well today***\r\n- Not finishing handling outliers\r\n***Things I will do better next time***\r\n- Finishing handling outliers\r\n\r\n***5/4/2024***\r\n\r\n***Things I learned today***\r\n- Debt can be positive for a company if debt for growth, debt for efficiency, debt for debt,tax benefits and positive market perception. Risks: increased debt, interest expenses, cash flow, risk perception, opportunity cost\r\n\r\n***Things I did well today***\r\n- Understanding that outliers detected are most likely not error outliers. Studying interesting outliers\r\n\r\n***Things I did not do well today***\r\n- Not finishing analysing and studying outliers\r\n- Errors with box plot programming\r\n\r\n***Things I will do better next time ***\r\n- Be specific in Trello about tasks\r\n- Continuing with outliers\r\n- Fixing errors with box plot programming\r\n\r\n***8/4/2024***\r\n***Things I learned today***\r\n- High vol trading: due to market news and events, earnings reports, price movements, trend confirmation, product launches, market sentiment shifts, algorithmic trading\r\n& institutional activity, context, innovation, breakouts and breakdowns, market orders and limit orders\r\n- In Python 3, zip built-in function wraps 2 or more iterators with a lazy generator (iterating over multiple iterators in parallel). => cleaner code than list, yielding tuples containing the next val from each generator. \r\nHowever, zip behaves strangely if input are of different lengths. \r\n\r\n***Things I did well today***\r\n- finishing box plots\r\n- analysing interesting outliers\r\n\r\n***Things I did not do well today***\r\n- Having not finishing interesting outlier analysis\r\n\r\n***Things I will do better next time***\r\n- Continuing interesting outlier analysis \r\n\r\n***10/4/2024***\r\n***Things I learned today***\r\n- Only create the axes and figure once\r\n***Things I did well today***\r\n- Continuing outlier analysis\r\n- Creating line plots\r\n***Things i did not do well today***\r\n- Line plots look messed up\r\n***Things I will do better next time***\r\n- Fixing up line plots\r\n- Learning more about loop\r\n\r\n***11/4/2024***\r\n***Things I learned today***\r\n- filtering operations data[col][data[col] > a] to remove unwanted parts\r\n***Things I did well today***\r\n- Analysing outliers\r\n- Understanding why behind outliers more\r\n***Things I did not do well today***\r\n- Not fixing up line plots\r\n- Not learning more about loops\r\n\r\n***16/04/2024***\r\n***Things I learned today***\r\n- PCA: a technique of orthogonal transformation converting correlated variables to non-linearly uncorrelated variables. \r\n- Price-related components might have more influence in PCA due to its variance in the market, its effects on investors due to price changes.\r\nVolume-related components might have less influence in PCA due to its relative stability\r\n\r\n***Things I did well today***\r\n- Analysing outliers\r\n- Understanding more about PCA\r\n\r\n***Things I did not do well today***\r\n- Not doing bollinger bands\r\n- Not doing time-series analysis\r\n- Not learning about loops\r\n\r\n***Things I will do better next time***\r\n- Continuing outlier analysis \r\n- Starting bollinger bands\r\n- Starting time-series analysis\r\n- Learning more about loops\r\n\r\n***1/5/2024***\r\n\r\n***Things I learned today***\r\n- Factors which could influence trading volume: earnings announcements, analysts ratings, market news and events, product launches and innovations\r\n\r\n***Things I did well today***\r\n- Continuing outlier analysis\r\n\r\n***Things I did not do well***\r\n- Not starting bollinger bands\r\n- Not starting time-series analysis\r\n- Not learning more about loops\r\n\r\n***3/5/2024***\r\n***Things I learned today***\r\n- 4 things could affect Volume: analyst ratings, earnings reports, news events and market trends.\r\n- Can use PatchTST to build a model\r\n***Things I did well today***\r\n- Nearly finishing outlier analysis\r\n- Chatting with Yudhi about my project\r\n***Things I did not do well today***\r\n- Going slow with outlier analysis\r\n***Things I will do better next time***\r\n- Speeding up outlier analysis\r\n\r\n***6/5/2024***\r\n***Things I learned today***\r\n- PayPal had outliers from 2013-2023 for price-related variables and from 2015-2023 for volume-related variables due to \r\nglobal expansion, new products and services, earnings reports\r\n***Things I did well today***\r\n- Continuing outlier analysis\r\n***Things I did not do well today***\r\n- No being as productive regarding outlier analysis\r\n***Things I will do better next time***\r\n- Not going to event next week during workday while working on my projects\r\n\r\n***8/5/2024***\r\n***Things I learned today***\r\n- .loc is label-based. Therefore, we need access row by equating our column to a value (where that value resides). Cannot .loc[value] since the \r\nlabel has to match the exact label, which is 'value'\r\n- Encountering IndexError today since there were empty objects when running code without exception handling\r\n***Things I did well today***\r\n- Continuing my outlier analysis \r\n- Understanding more about .loc \r\n***Things I did not do well today***\r\n- Having not finished outlier analysis\r\n- Having taken some time to figure IndexError\r\n\r\n***Things I will do better next time***\r\n- Giving myself a timeframe to figure out error (30 mins). If not, check answer with Copilot then move on\r\n- Continuing with outlier analysis\r\n\r\n**13/5/2024***\r\n***Things I learned today***\r\n- Bollinger Band: normally for short-term analysis of volatility (over or under-valued). 3 bands: upper band: set of certain number of SD (usually 2 above the MB), middle band (20 day SMA (Simple Moving Average)) and lower band (MB - 2 SD)\r\n\r\n***Things I did well today***\r\n- Finishing outlier analysis\r\n- Looking into problem identification, investigation, formulation, and prevention\r\n\r\n***Things I did not do well today ***\r\n- Not finishing out problem identification\r\n- Rereading previous sentences\r\n***Things I will do better next time ***\r\n- Reading sentences carefully \r\n- Continuing with problem identification\r\n\r\n***10/6/2024***\r\n***Things I learned today***\r\n- moving average is a stock indicator, a type of times series analysis, for technical analysis. 3 types: simple moving average, cumulative average, exponential average\r\nDepends on use cases, use which one for best insights\r\n***Things i did well today***\r\n- Started time series analysis\r\n***Things I did not do well today***\r\n- Not finishing moving average\r\n***Things I will do better next time***\r\n- Finishing moving average\r\n\r\n***11/6/2024***\r\n***Things I learned today***\r\n- To determine the best smoothing technique, check: trend, noise, seasonality, volatility, missing values and outliers\r\n***Things I did well today***\r\n- Looking at different smoothing techniques\r\n***Things I did not do well today***\r\n- Not able to choose a smoothing technique\r\n***Things I will do better next time***\r\n- Choose a suitable smoothing technique\r\n\r\n***12/06/2024***\r\nhttps://www.strike.money/technical-analysis/volatility-analysis\r\nhttps://www.investopedia.com/articles/basics/09/simplified-measuring-interpreting-volatility.asp\r\n***Things I learned today***\r\n- to check if data follow normal distribution: use various techniques\r\n- Q-Q plot: check central tendency, deviations, and overall trend\r\n\r\n***Things I did well today***\r\n- rechecked my normal distribution code\r\n***Things I did not do well today***\r\n- not finished check normal distribution\r\n***Things I will do better next time***\r\n- Finishing detection for normal distribution\r\n\r\n13/6/2024\r\n***Things I learned today***\r\n- Since stock data tend to have fat tails (more extreme values), we should not choose Shapiro-Wilk test for normality. Instead,\r\nwe should opt for Kolmogorov-smirnov test or anderson-darling test\r\n***Things I did well today***\r\n- conducted the kolmogorov-smirnov test and anderson-darling test\r\n***Things I did not do well today***\r\n- Not deciding the tests for volatility \r\n***Things I will do better next time***\r\n- Choosing tests for volatility and starting to conduct them \r\n\r\n14/06/2024\r\n***Things I learned today***\r\n- Volatility test: Average True Range, Bollinger bands, Implied Volatility\r\n***Things I did well today***\r\n- Started volatility test\r\n***Things I did not do well today***\r\n- Not finishing volatility test\r\n***Things I will do better next time***\r\n- Finishing volatility tests\r\n\r\n15/06/2024\r\n***Things I learned today***\r\n- To choose a volatility threshold for ATR, investigate: trading strategy, risk tolerance, asset characteristics,\r\nand historical volatility. ATR can help to identify high volatility phases, set stop loss levels (can times 2 or 3 using the average historical volatility), filter out market\r\nnoise, position sizing (adjusting positions)\r\n\r\n***Things I did well today***\r\n- Finished coding for ATR\r\n\r\n***Things I did not do well today***\r\n- Not finishing volatility test\r\n***Things I will do better next time***\r\n- Finishing volatility test\r\n\r\n\r\n18/6/2024\r\n***Things I learned today***\r\n- to conduct volatility test, need real-time data\r\n***Things I did well today***\r\n- Understand the volatility test\r\n- Start obtaining real-time data\r\n***Things I did not do well today***\r\n- Did not finish real-time data analysis\r\n***Things I will do better next time***\r\n- Finishing real-time data analysis\r\n\r\n19/06/2024\r\n***Things I learned today***\r\n- can build reflection agent: generator and reflector\r\n***Things I did well today***\r\n- Looking at building a reflection agent\r\n***Things I did not do well today***\r\n- Error with importing LangGraph\r\n***Things I will do better next time***\r\n- Looking at the error more closely\r\n20/06/2024\r\n***Things I learned today***\r\n- building Markov-chain -based multi-agent to reduce hallucination\r\n***Things I did well today***\r\n- Solve LangGraph error by importing module httpx\r\n- Looking at building a reflexion agent using Gemini and using Markov Chain\r\n***Things I did not do well today***\r\n- Not finishing building a reflexion agent\r\n- Not reading the reflexion paper\r\n- Not fully understanding error messages\r\n***Things I will do better next time***\r\n- Read the reflexion paper\r\n- continue building a reflexion paper\r\n- Understand error messages better\r\n21/06/2024\r\n***Things I learned today***\r\n- Reflexion: a novel framework to reinforce the agent not by updating the weights but by giving linguistic feedback. LLM agents find it hard\r\nto learn from past mistakes\r\n- getpass: a module to process passwords\r\n- -> None: return no value\r\n- (var:str): variables should be strings\r\n***Things I did well today***\r\n- Finished the Going Meta: Building a reflection agent with LangGraph\r\n- Started reading \"Reflexion: Language Agents with Verbal Reinforcement Learning\"\r\n***Things I did not do well today***\r\n- Not finishing reading the paper\r\n- Not finishing building the agent\r\n***Things I will do better next time***\r\n- Finishing reading the paper\r\n- Continue building the agent\r\n22/6/2024\r\n***Things I learned today***\r\n- Knowledge Graph + Multimodal learning => overcome challenges, heading towards AGI\r\n***Things I did well today***\r\n- finished common UI Patterns\r\n- nearly finished Information Architecture\r\n- started reading \"Knowledge Graph Meets Multimodal Learning: A Comprehensive Survey\"\r\n***Things I did not do well***\r\n- Not finished IA yet\r\n- Not finished reading the paper yet\r\n***Things I will do better next time***\r\n- Continuing reading the paper\r\n- Finishing IA and UI/UX design\r\n\r\n24/6/2024\r\n***Things I learned today***\r\n- MMKG => KG is multimodal if knowledge symbols contain multiple modalities.\r\n- 2 approaches to MMKG construction: 1=> labelling KG symbols with images, 2=> grounding KG symbols to images\r\n***Things I did well today***\r\n- continue reading the paper KG Meets Multimodality\r\n- Coding to configure tracing\r\n***Things I did not do well today***\r\n- Not finishing the paper\r\n- Not finishing configure tracing\r\n***Things I will do better next time***\r\n- Reading the paper faster\r\n- Finishing tracing \r\n- Start looking at building a multimodal AI agent\r\n\r\n25/6/2024\r\n***Things I learned today***\r\n- 4 types of tasks: understand and reasoning tasks (VQA and VQG), classification tasks, content generation tasks, retrieval tasks\r\n\r\n***Things I did well today***\r\n- Reading MMKG paper\r\n- Coding the agent\r\n***Things I did not do well today***\r\n- Not finishing MMKG paper\r\n- Trouble with creating prompt\r\n***Things I will do better next time***\r\n- Reading the paper faster \r\n- Look at prompt engineering for multimodal model. \r\n\r\n26/6/2024\r\n***Things I learned today***\r\n- Multiple types of tasks for MMKGs\r\n- top_k: method using the top probability k in words\r\n- top_p: the size of the shortlist of words based on the sum of likelihood score based on sum threshold\r\n***Things I did well today***\r\n- Finishing the second pass\r\n- Creating prompt\r\n***Things I did not do well today***\r\n- Error with prompt\r\n***Things I will do better next time***\r\n- Understand Gemini model \r\n- Understand the prompts for Gemini\r\n\r\n27/06/2024\r\n***Things I learned today***\r\n- memories divided to 2 types: conditioned reflexes (behaviors learned from experience) and torso-to-tail knowledge (head:common knowledge, torso: less common, tail: least common)\r\n- can combine crewAI with Gemini\r\n***Things I did well today***\r\n- third pass of MMKG\r\n- continue building agents\r\n***Things I did not do well today***\r\n- error with Agent\r\n***Things I will do better next time***\r\n- read crewAI docs\r\n- fix the agent API code\r\n\r\n1/7/2024\r\n***Things I learned today***\r\n- attention mechanism used in transformer based model \r\n***Things I did well today***\r\n- access Gemini model from torch library\r\n- continued reading MMKG\r\n***Things I did not do well today***\r\n- Not finished 3rd-pass reading MMKG\r\n***Things I will do better next time***\r\n- Reading the paper first for the 3rd pass\r\n- Focusing on relevant papers \r\n- Challenging assumptions of the paper\r\n- Continue coding for Gemini\r\n\r\n2/7/2024\r\n***Things I learned today***\r\n- KGs => graph algos, graph nn. KGs help both humans and machines digest data and relationships \r\n- for unit testing => use pytest\r\n- types of software testing: unit testing, integration testing, system testing, acceptance testing \r\n***Things I did well today***\r\n- start unit testing for my code\r\n- continue 3rd pass reading and challenging the paper\r\n***Things I did not do well today***\r\n- not finishing testing yet since error of baseline_images not found. \r\n- not finishing 3rd pass reading\r\n***Things I will not do well today***\r\n- Fix errors\r\n- finish unit testing\r\n- continue 3rd pass reading and challenge the paper \r\n5/7/2024\r\n***Things I learned today***\r\n- install matplotlib from the source for test data\r\n***Things I did well today***\r\n- fixing errors\r\n***Things I did not do well today***\r\n- not finishing fixing\r\n***Things I will do better next time***\r\n- continue fixing the error\r\n\r\n8/7/2024\r\n***Things I learned today:***\r\n- Advantages of initialising parameters: breaking symmetry, avoiding exploding/vanishing gradients, faster convergence. Disadvantages: stuck at local optima, \r\nexploding values,heuristics for initial scale or weights\r\n***Things I did well today:***\r\n- Creating agents\r\n- Continue 3rd-pass reading of MMKG\r\n***Things I did not do well today:***\r\n- Not fixing unit testing error in pytest\r\n***Things I will do better next time:***\r\n- Fixing unit testing error in pytest \r\n- Asking Hayden\r\n\r\n9/7/2024\r\n***Things I learned today***\r\n- fl-mmkg: feature-level knowledge graphs, features represent multimodal data. Advantages: flexible, improved performance, rich information, interpretable. drawbacks: complexity, noise, scalability\r\n- n-mmkg: nodes (entities) represent mm data. Advantages: rich info, improved performance, scalability, flexibility, interconnected entities. drawbacks: limited modalities since each node == each modality, integration challenge, performance\r\n***Things I did well today***\r\n- 3rd pass MMKG\r\n- constructing fl-mmkg\r\n***Things I did not do well today***\r\n- not fixing the error\r\n- still 3rd pass MMKG => not fully understanding the paper\r\n- constructing fl-mmkg\r\n***Things I will do better next time***\r\n- ask Hayden about the error\r\n- look the level of understanding of the paper. look at background papers\r\n- continue to look at constructing fl-mmkg \r\n\r\n10/7/2024\r\n***Things I learned today***\r\n- harmonic mean: the reciprocal of the arithmetic mean of reciprocals, used in F1 score (P x R => to balance between P and R)\r\n\r\n***Things I did well today***\r\n- looking at how to collect data \r\n- looking at fixing pytest error\r\n- 3rd pass reading MMKG\r\n***Things I did not do well today***\r\n- not able to fix pytest error\r\n- not finishing 3rd pass yet\r\n\r\n***Things I will do better next time***\r\n- ask Copilot\r\n- continue looking at data collection\r\n- keep challenging and looking at background papers for MMKG\r\n\r\n11/7/2024\r\n***Things I learned today***\r\n- lemmatization helps find the root of words, making conversations easier for an AI agent to understand\r\n***Things I did well today***\r\n- developing uri tool\r\n***Things I did not do well today***\r\n- not able to finish 3rd pass reading\r\n- not able to solve error\r\n***Things I will do better next time***\r\n- Finish 3rd pass reading\r\n- ask someone to solve errors\r\n\r\n15/07/2024\r\n***Things I learned today***\r\n- compound structure: a data structure composed of different data types. \r\n- main operator: the main operator of the data structure\r\n- assert: check if a condition is met, otherwise throw an assertion error\r\n***Things I did well today:***\r\n- added compound_uri, is_absolute_uri, join_uri,concept_uri, uri_prefix,split_uri functions\r\n- finished 3rd pass MMKG\r\n***Things I did not do well today:***\r\n- did not fully understand the MMKG paper after 3rd pass. \r\n***Things I will do better next time***\r\n- Will need to read background papers or do more research on MMKG\r\n- Asking about the unit test error\r\n\r\n16/07/2024\r\n***Things I learned today:***\r\n- yield: keyword to create a generator function (behave like a iterator) \r\n- a compound URI: contains an operator and list of arguments\r\n- conjunction of sources: combining multiple sources to create an assert URI\r\n- Advantages of ConceptNet: multilingual, broad coverage, word embeddings, open-source. Disadvantages: complexity, data quality. lack of practical detail\r\nTo improve: enhance data quality, expand coverage, improve performance with data structures and algorithms, enhance usability (documentation,tutorials,new tools, libraries),\r\nresearch (use it as a basis for researching)\r\n***Things I did well today:***\r\n- adding functions to uri tooling: is_concept, is_relation, is_term, uri_prefixes,conjunction_uri, parse_compound_uri,parse_possible_compound_uri,get_uri_language\r\n- looking at deploying app on GC\r\n- knowing more about ConceptNet\r\n***Things I did not do well today:***\r\n- not reading background papers on MMKG\r\n- not asking about unit test error\r\n***Things I will do better next time***\r\n- asking about unit test error\r\n- reading background papers on MMKG\r\n\r\n17/07/2024\r\n***Things I learned today:***\r\n- ConceptNet\r\n***Things I did well today:***\r\n- adding add_tools package\r\n- starting to read Knowledge Graphs paper\r\n***Things I did not do well today:***\r\n- not looking at wireframing\r\n- not fixing unit test error\r\n***Things I will do better next time:***\r\n- looking at wireframing\r\n- fixing unit test error\r\n- continuing KG construction\r\n\r\n22/7/2024\r\n***Things I learned today***\r\n- to access a package in a directory, we might need to add the parent package when importing \r\n***Things I did well today***\r\n- fixed the error of importing uri\r\n- finished the function add_relation\r\n***Things I did not do well today***\r\n- Spending a lot of time fixing the import\r\n***Things I will do better next time***\r\n- Reading MMKG background paper\r\n- Continuing MMKG construction\r\n- Looking at wireframing \r\n\r\n23/07/2024\r\n***Things I learned today:***\r\n- re: a built-in module to work with regular expressions (characters matching specific patterns).\r\nre.match: deciding if the regex matches at the beginning of the string\r\nre.findall: finding all non-overlapping matchies of the regex as a list of strings\r\n- exception: thrown when request is unsuccessful. try: attempting a block of code before throwing an error\r\n- exponential backoff: an error-handling strategy to increase delayed time exponentially.\r\nrandomised exponential backoff: starting from 0 and finishing at exponential backoff, varying the delayed time to to prevent clients starting at the same time to resend requests repeatedly.\r\n***Things I did well today:***\r\n- coding for fetch_tool\r\n- trying to deploy the app\r\n***Things I did not do well today:***\r\n- Not deploying yet since no app yet\r\n- No wireframing\r\n- No MMKG background paper\r\n***Things I will do better next time:***\r\n- Continue to build MMKG\r\n- Looking at wireframing when have time\r\n- Reading MMKG paper if have time\r\n\r\n24/7/2024\r\n***Things I learned today***\r\n- pattern\r\n- need proper exception handling\r\n- pagination: node data across different pages\r\n- knowledge graph: entity, edge, node\r\n***Things I did well today:***\r\n- added transform_path, get_page, get_node, combine_pages, get_edge, get_multi_model\r\n- read the knowledge graph paper\r\n***Things I did not do well today:***\r\n- no wireframing\r\n- not understanding IOError\r\n***Things I will do better next time***\r\n- wireframing if have time\r\n- Understanding IOError\r\n- Continuing reading knowledge graph paper\r\n\r\n25/07/2024\r\n***Things I learned today:***\r\n- space and time complexity: how much time does it take to run and how much space memory does it take up\r\n- a set is quicker to look up since it uses a hash table, whereas a list needs to check first \r\n- weights (graph): the weights of the element (related to strength, importance, relevance, confidence). Strategies to determine: frequency-based, attribute-based\r\n***Things I did well today:***\r\n- adding acceptable_element, save_to_local\r\n- reading KG paper\r\n- Understanding IOError\r\n***Things I did not do well today:***\r\n- no wireframing\r\n- read KG paper slowly\r\n- not determined the weights strategy yet\r\n***Things I will do better next time:***\r\n- wireframing if have time\r\n- continuing KG paper \r\n- determining the strategy for weights\r\n\r\n26/07/2024\r\n***Things I learned today:***\r\n- subprocess: module to spawn new processes from given input/output/error pipelines\r\n***Things I did well today:***\r\n- adding delete_uri, filter_node, need_extension\r\n***Things I did not do well today:***\r\n- not determining the weight strategy\r\n- no wireframing\r\n***Things I will do better next time:***\r\n- wireframing if have time\r\n- determining the weight strategy\r\n- continue coding\r\n- reading KG paper\r\n\r\n29/07/2024\r\n***Things I learned today***\r\n- Identity: global identifiers, identity links, persistent identifiers, lexicalisation\r\n- Context: direct representation, higher-arity representation, reification, annotation\r\n***Things I did well today***\r\n- reading KG\r\n- coding for downloading images\r\n***Things I did not do well today***\r\n- spending too much time on search url\r\n- no wireframing\r\n- no weight strategy\r\n***Things I will do better next time***\r\n- spending less time on search url\r\n- no weight strategy\r\n- wireframing when we can\r\n\r\n30/07/2024\r\n***Things I learned today:***\r\n- requests.get: Send a GET request\r\n- .format: formatting a string\r\n- .raise_for_status: check if the request is successful, raise an error if unsuccessful\r\n- wb: writing binary mode\r\n- ontologies in KG: formal representation of knowledge so that we represent data as a graph => interpretations, properties, individuals, classes\r\n***Things I did well today:***\r\n- continuing coding to download sound\r\n- continuing to read KG paper\r\n***Things I did not do well today:***\r\n- not finished download_sound. \r\n- no wireframing\r\n***Things I will do better next time:***\r\n- Finishing download_sound\r\n- Wireframing if can\r\n- Continuing to read KG paper\r\n\r\n31/07/2024\r\n***Things I learned today***\r\n- subprocess.check_output: run a command and give output\r\n- subprocess.Popen : starting a process to run a specific command\r\n- process.communicate: sending a process and return its output and error\r\n- process.kill: terminate the process immediately\r\n- process.terminate: sending a signal to terminate the process \r\n\r\n***Things I did well today***\r\n- finish coding download_sounds\r\n- read KG paper\r\n\r\n***Things I did not do well today***\r\n- not finished extend_tool\r\n- not wireframed\r\n***Things I will do better next time***\r\n- finishing extend_tool\r\n- wireframing \r\n- reading KG paper\r\n\r\n1/8/2024\r\n***Things I learned today:***\r\n- ensuring that the return type is consistent \r\n- need logging to help with tracking. logging.basicConfig() => setting up basic configurations. logging.info() => Emitting log messages. logging.error => Emitting error messages if serious problems. \r\n- 3 types of basic elements in Description Logics: individuals, classes and properties\r\n***Things I did well today:***\r\n- Coding functions for extend tool\r\n- Wireframing\r\n- Reading KG paper\r\n\r\n***Things I did not do well today:***\r\n- Not finished extend_tool\r\n\r\n***Things I will do better next time:***\r\n- Finishing extend_tool \r\n- Continuing to wireframe\r\n- Reading KG paper\r\n\r\n2/8/2024\r\n***Things I learned today:***\r\n- List, Dict, Any => typing\r\n***Things I did well today:***\r\n- coding\r\n- wireframing\r\n***Things I did not do well today:***\r\n- not finished wireframing\r\n- not read KG paper\r\n***Things I will not do better today:***\r\n- Continue coding\r\n- Finishing wireframing\r\n- Reading KG paper\r\n\r\n5/8/2024\r\n***Things I learned today:***\r\n- input prompt should be outside the function as a good practice\r\n- demo video: should know what to say and what product does. 1) plan the video. 2) record your product or service in action 3)edit video\r\n***Things I did well today:***\r\n- finished download visuals\r\n- looked at creating demo\r\n***Things I did not do well today:***\r\n- no wireframing\r\n- no KG paper reading\r\n***Things I will do better next time:***\r\n- continue to code\r\n- wireframing if can\r\n- continue reading KG paper\r\n\r\n6/8/2024\r\n***Things I learned today***\r\n- global variable: a variable, declared outside, with global scope across functions and classes. Characteristics: global scope, persistence, shared state. When to use: config settings, constants, shared state. Disadvantages: unintended modifications, reduced readability (having to find the global var), testing difficulties. Good practices: minimise use, encapsulation, clear naming.\r\n***Things I did well today:***\r\n- Coding for transport tool.\r\n- Understanding what global variable is.\r\n***Things I did not do well today:***\r\n- No reading KG paper\r\n- Not finished transport tool\r\n***Things I will do better next time:***\r\n- Understanding command\r\n- Reading KG paper\r\n- Continuing coding transport tool\r\n\r\n7/8/2024\r\n***Things I learned today***\r\n- Need transport tool to package and send a file to a server\r\n***Things I did well today:***\r\n- Finished the transport tool package\r\n***Things I did not do well today:***\r\n- No wireframing\r\n- No reading KG paper\r\n***Things I will do better next time:***\r\n- Wireframing if possible \r\n- Reading KG paper if possible \r\n- Continue coding \r\n- Understanding more deeply\r\n\r\n9/8/2024\r\n***Things I learned today:***\r\n- need to clean up and delete graphs because of: data integrity, testing nodes and graphs, performance, security, consistency\r\n- except ServiceUnavailable: an exception when service is unavailable\r\n\r\n***Things I did well today:***\r\n- nearly finished prototyping\r\n- continued coding\r\n***Things I did not do well today:***\r\n- No wireframing\r\n- No KG reading\r\n- Not fully focused\r\n\r\n***Things I will do better next time:***\r\n- Focusing on prototyping (practice focus with meditation)\r\n- Continue to code the rest of the knowledge graph \r\n\r\n13/08/2024\r\n***Things I learned today:***\r\n- json.decoder.JSONDecodeError: throwing an exception when decoding JSON in file\r\n\r\n***Things I did well today:***\r\n- coding history.py\r\n\r\n***Things I did not do well today:***\r\n- few code\r\n\r\n***Things I will do better next time:***\r\n- continue to practise coding everyday \r\n\r\n2/9/2024\r\n***Things I learned today:***\r\n- need to add the module to the path sys.path.append('C:/Users/laran/PycharmProjects/ASX/stock_market/fl_multimodal_knowledge_graph/data_collection/data_collection_tools') \r\nto use download_sounds in the test\r\n- difference between assertions (assumptions during the executions of code) and exceptions (runtime errors and exceptional conditions)\r\n***Things I did well today:***\r\n- created a unit test for download_sounds\r\n- added mel_spectrogram_features to the code feature_extractor to convert audio wave to mel spectrogram for better signal-to-noise ratio (adding filter)\r\n***Things I did not do well today:***\r\n- caught mistakes in download_sounds module while conducting a unit test\r\n- not finished mel_spectrogram_features yet\r\n***Things I will do better next time:***\r\n- understanding where the errors came from (root cause)\r\n- reading knowledge graph paper\r\n- continue with mel_spectrogram_features\r\n\r\n3/9/2024\r\n***Things I learned today:***\r\n- The consistent TypeError is due to no proxy_list passed. I expected this since I don't want to pass a proxy list yet\r\n- a decorator is a function to modify behaviors without changing the actual code. Characteristics: functions as arguments, wrapper function, syntatic sugar. Use cases: logging, authentication, timing\r\n- While pytest is capturing output, the input function cannot read from stdin(standard inputs) => need to mock the input function to a predefined valu\r\n***Things I did well today:***\r\n- Figured out the TypeError and why it happened\r\n- Completed unit tests for download_sounds and download_images\r\n\r\n***Things I did not do well today:***\r\n- Took too much time to figure out TypeError\r\n\r\n***Things I will do better next time:***\r\n- Find a way to understand the root cause of errors better\r\n- Read the Knowledge Graph paper\r\n- Wireframing while can \r\n\r\n4/9/2024\r\n\r\n***Things I learned today:***\r\n- Framing: create frames from samples for better sampling, window => samples within each frame, hop => advance to move forward from 1 frame to another\r\n- Hann window: window that has hann function applied to make it smoother and prevent spectral leakage with better Fourier analysis\r\n- padding: adding zeros to arrays to make the length of each frame the same\r\n- Short-Time Fourier Transform (STFT): allowing for the analysis of signals by dividing longer signals in to smaller ones. STFT{x(t)}(m,omega) = summation (from n = - infinity to inf) of x[n].w[n-m].e to the power of j omega n\r\n- Magnitude of STFT: the amplitude of the frequency component at each point in time. = |STFT{x(t)}(m,omega)|\r\n\r\n***Things I did well today***\r\n- Coding framing, Hanning window and stft magnitude\r\n- Reading KG papers\r\n- Understanding more about Fourier Transform\r\n***Things I did not do well today:***\r\n- Not finished mel_spectrogram_features module yet\r\n- No wireframing yet\r\n- not finding a way to understand the root cause of errors better yet\r\n- Not writing unit tests yet\r\n\r\n***Things I will do better next time:***\r\n- Understand each and every line of code and its concept\r\n- Reading KG papers\r\n- Wireframing if have time \r\n- Finding a way to understand root cause of errors \r\n- Writing unit tests where applicable\r\n\r\n5/9/2024\r\n***Things I learned today***\r\n- converting from hertz to mel is beneficial since it mimics human hearing and help AI agents to hear like humans do due to better perceptual alignment, better feature extraction, better analysis, model performance and data efficiency\r\n- Nyquist frequency: the highest frequency in a digital processing system without aliasing (distortion)\r\n- Need to add ValueError when checking to see if values are incorrectly ordered or out of range (suitable for checking values like Hertz, kgs etc)\r\n\r\n***Things I did well today：***\r\n- Adding hertz_to_mel(), spectrogram_to_mel_matrix(), log_mel_spectrogram()\r\n- Understanding more about the process of transforming audio waveform to mel spectrogram features \r\n\r\n***Things I did not do well today:***\r\n- Not understanding why we need to log mel spectrogram\r\n- Not finishing stft_magnitude()\r\n- Not reading knowledge graph paper\r\n- Not wireframing\r\n\r\n***Things I will do better next time：***\r\n- Spending an hour next week for wireframing\r\n- Understanding the need for logging mel spectrogram\r\n- Finishing stft_magnitude()\r\n- Reading KG paper\r\n- Looking at ways to understand root cause of errors\r\n\r\n9/9/2024\r\n***Things I learned today:***\r\n- Real Fast Fourier Transform: specialised version of FFT for real-valued input sequences.\r\n- logging mel spectrogram to make large and small values more manageable (dynamic range compresion), create a representation that is more aligned with human hearing because of better feature extraction (perceptual relevance), avoid negative infinites by adding a small constant and improve convergence, and enhance feature discrimination using feature scaling and reduce noise\r\n\r\n***Things I did well today:***\r\n- Finish mel_spectrogram_features module\r\n- Looking at image extraction\r\n***Things I did not do well today:***\r\n- Not reading KG papers\r\n- Not wireframing \r\n- Not looking at ways to understand root cause errors\r\n\r\n***Things I will do better next time:***\r\n- Wireframe tomorrow\r\n- Reading KG if have time\r\n- Look at ways to understand root cause of errors\r\n\r\n10/09/2024\r\n***Things I learned today:***\r\n- Using KL divergence to control sparsity level, more flexibility albeit more complex. Use L1 regularisation for simplicity, standard\r\n- Combining CNNs and Vision Transformers as a base architecture may improve F1 scores. Hierarchical labels may also improve F1 scores. PreSizer resizes images, uses reflective padding, doesn't remove meaningful info, removes noise around images => improves F1 scores. Resolution is important for fine-grained classification tasks\r\n- Improving Transformer-based model with quantization, knowledge distillation, pruning, low-rank approximation (weight matrices and low-rank matrices)\r\n- ***Things I did well today:***\r\n- Choosing base architecture for image extraction\r\n- wireframing\r\n***Things I did not do well today:***\r\n- Not understanding what low-rank approximation is yet\r\n- Not reading KG paper\r\n- Not finishing sparse autoencoder\r\n***Things I will do better next time:***\r\n- Understanding low-rank approximation\r\n- reading KG paper\r\n- looking at ways to understand root cause of errors\r\n- Finishing sparse autoencoder\r\n- Starting ConVit\r\n\r\n11/09/2024\r\n\r\n***Things I learned today:***\r\n- Using tensorflow for TPU, ease of support, community and with Keras API (ease of use, modularity, pretrained model, community and support)\r\n- Using StratifiedGroupKfold for grouped data (companies in this case) to ensure representation for each group, no imbalance, each group appears once exactly in all folds (evaluating on unseen companies)\r\n- Using sigmoid as an activation function for reconstruction ranging from 0 to 1=> loss: binary entropy\r\n***Things I did well today:***\r\n- Coding sparse autoencoder to learn feature representations and look at interpretability \r\n- Learning how to use tensorflow\r\n- Looking at ways of understanding root causes of errors \r\n***Things I did not do well today:***\r\n- Not finished coding for sparse autoencoder \r\n- Not reading KG paper\r\n- Not starting Convit\r\n- Not understanding low-rank approximation\r\n***Things I will do better next time:***\r\n- Finishing sparse autoencoder\r\n- Reading KG paper\r\n- Understanding low-rank approximation\r\n- Starting ConVit\r\n\r\n12/9/2024\r\n***Things I learned today:***\r\n- Can use keras_tuner for hypertuning \r\n\r\n***Things I did well today:***\r\n- Coding Sparse autoencoder\r\n- Using keras_tuner\r\n- Finished paper Convit\r\n- Learning new techniques\r\n***Things I did not do well:***\r\n- Not reading KG\r\n- not understand low-rank approximation\r\n- Not finishing Sparse autoencoder\r\n***Things I will do better next time:***\r\n- reading KG\r\n- understanding low-rank approximation\r\n- finishing sparse autoencoder\r\n- starting ConVit\r\n\r\n13/09/2024\r\n***Things I learned today:***\r\n- metrics for image tasks: mse, mae, psnr (Peak signal-to-noise ratio, measuring the difference between the highest value of signal to the power of corrupting noise),ssim(structural similarity index, measuring the difference between 2 images based on luminance, contrast and structure)\r\n- \r\n***Things I did well today:***\r\n- Finishing sparse autoencoder\r\n- starting ConVit\r\n- kind of understanding low-rank approximation \r\n\r\n***Things I did not do well today:***\r\n- Not finishing MLP\r\n- Not fully understanding low-rank approximation\r\n- Not reading KG\r\n***Things I will do better next time:***\r\n- Finishing MLP\r\n- Fully understanding low-rank approx\r\n- Reading KG paper\r\n\r\n16/09/2024\r\n***Things I learned today***\r\n- low-rank approximation: approximating a given matrix using a different matrix of a lower rank for data compression, noise reduction and feature extraction, via Singular Value Decomposition(SVD)\r\n- multi-layer perceptron: a type of nn with 3 layers, using feedforward propagation, backpropagation and activation function. A dropout layer is optional to prevent overfitting\r\n- a gated positional self-attention: self-attention with a gating mechanism to focus on nearby groups of inputs\r\n***Things I did well today:***\r\n- finished MLP\r\n- read KG paper\r\n- understood low-rank approximation better\r\n***Things I did not do well today:***\r\n- not finished gpsa\r\n***Things I will do better next time:***\r\n- finished gpsa \r\n- adding complex beacons to make sure the code is readable\r\n- ensure I understand the math behind gpsa\r\n- include feature importance\r\n- incorporate other types of AI models\r\n\r\n17/09/2024\r\n***Things I learned today:***\r\n- if not hasattr: check condition if has attribute\r\n\r\n***Things I did well today:***\r\n- continued convit\r\n- read KG paper\r\n- starting to understand the math behind GPSA\r\n***Things I did not do well today:***\r\n- not including feature importance\r\n- not incorporating other types of AI models\r\n***Things I will do better next time:***\r\n- including feature importance \r\n- continuing convit\r\n- reading KG paper\r\n- understand the math behind GPSA\r\n- incorporating other types of AI models\r\n\r\n18/09/2024\r\n\r\n***Things I learned today:***\r\n- the GPSA includes: calculating attention heads, obtaining an attention map, calculating positional scores, calculating patch scores, forward pass. calculating relative indices \r\n\r\n***Things I did well today:***\r\n- finished GPSA \r\n- read Knowledge Graph\r\n- understanding more about the math behind GPSA\r\n\r\n***Things I did not do well today:***\r\n- not included feature importance\r\n- not incorporated other types of AI models\r\n\r\n***Things I will do better next time:***\r\n- including feature importance \r\n- incorporating other types of AI models\r\n- understanding the concept behind attention mechanism: how query, key, and value vectors computed; how attention scores computed and used to weigh the value vectors; understand positional encoding (how it is used, and the role of relative indices), understand softmax function (how it is used to normalised, how softmax ensures att scores sum to one) understanding gating mechanism (combining patch scores and positional scores, how sigmoid control the influences of each type of score), matrix multiplications (matrix ops for computing attn scores, how these ops are implemented for efficient coding), how to implement GPSA, how to debug and optimize  \r\n\r\n19/09/2024\r\n***Things I learned today:***\r\n- multi-head self-attention: a module used in transformer model, self-attention: how a model focuses on words in a sequence, multi-head: how the model distributes attention\r\n\r\n***Things I did well today:***\r\n- read \"Attention is all you need\"\r\n- coding MHSA\r\n\r\n***Things I did not do well today:***\r\n- did not finish MHSA\r\n- did not finish paper\r\n\r\n***Things I will do better next time:***\r\n- understand the paper more\r\n- read KG\r\n- include feature importance \r\n- incorporate other types of AI models\r\n- finish MHSA\r\n\r\n20/09/2024\r\n\r\n***Things I learned today:***\r\n- Euclidean distance: measuring the distance between two points, in units, based on Pythagorean theorem\r\n- incorporating other types of AI models depends on the problems at hand. Pros: enhanced performance, improved generalisation, task-specific optimisation, flexibility and adaptability. Cons: complexity, integration, evaluation\r\n\r\n***Things I did well today:***\r\n- finished MHSA\r\n- read paper “Attention is all you need\"\r\n\r\n***Things I did not do well today:***\r\n- not including feature importance\r\n- not incorporating other types of AI models\r\n\r\n***Things I will do better next time:***\r\n- include feature importance\r\n- incorporate other types of AI models\r\n- read KG paper if have time\r\n- read paper \"Attention is all you need\"\r\n\r\n23/09/2024\r\n\r\n***Things I learned today:***\r\n- 3 reasons of self-attention in Tranformer: reducing computational complexity per layer, ensuring every computation is parallised, reducing the length of path between long-range dependencies\r\n- flatten: collapsing dimensions\r\n- backbone of CNN: can be used to extract features\r\n- kernel: a filter, a matrix to multiply with image for model to pay attention to specific features\r\n\r\n***Things I did well today:***\r\n- Coding HybridEmbed\r\n- Finished second pass \"Attention is all you need\"\r\n- understand more about the architecture of ConviTransformer\r\n\r\n***Things I did not do well today:***\r\n- Not reading KG paper\r\n- Not finished convit yet\r\n\r\n\r\n***Things I will do better next time:***\r\n- 3rd pass Attention paper\r\n- Finished convit\r\n- feature importance\r\n- other types of AI models\r\n- KG paper if have time\r\n\r\n24/09/2024\r\n***Things I learned today:***\r\n- SHAP (Shapely Additive exPlanations): visualising outputs from models and assigning Shapely values\r\n- hybrid backbone: backbone models use a combination of different neural networks\r\n\r\n***Things I did well today:***\r\n- finished Conviformers\r\n- applied SHAP\r\n- implementing \"Attention is all you need\"\r\n\r\n***Things I did not do well today:***\r\n- not reading KG paper\r\n- not incorporating other types of AI models\r\n\r\n***Things I will do better next time:***\r\n- Reading KG paper if have time\r\n- Incorporating other types of AI models\r\n\r\n25/09/2024\r\n***Things I learned today***\r\n- hidden markov model: each state depends on the prev state\r\n\r\n***Things I did well today***\r\n- Hidden Markov Model\r\n- code to download texts\r\n- 3rd pass attention\r\n- read KG\r\n\r\n***Things I did not do well today***\r\n- not finished 3rd pass attention\r\n\r\n***Things I will do better next time***\r\n- finish 3rd pass attention\r\n- continue coding to download texts\r\n- create account to obtain APIs\r\n\r\n26/09/2024\r\n***Things I learned today:***\r\n- To obtain listed companies' name, we can use stock exchange websites. \r\n\r\n***Things I did well today:***\r\n- learned how to use Twitter API\r\n- learned how to obtain information via coding\r\n- finished 3rd pass \"Attention is All You Need\"\r\n\r\n***Things I did not do well today:***\r\n- not finished coding regarding Twitter API\r\n- not reading Hidden Markov Model\r\n- not reading KG paper\r\n\r\n***Things I will do better next time:***\r\n- finish coding regarding Twitter API\r\n- reading Hidden Markov Model\r\n- reading KG paper\r\n\r\n27/09/2024\r\n***Things I learned today:***\r\n- Inspecting HTML elements to see what corresponds to company's name. \r\n- HMM consists of a state transition model, an observation model, and a state distribution\r\n***Things I did well today:***\r\n- Adding add_multimodal_text_node\r\n- Looking at how to access listed companies' names via stock exchange websites\r\n- Reading HMM tutorials\r\n***Things I did not do well today:***\r\n- Not reading KG paper\r\n- Not finished coding regarding Twitter API yet\r\n***Things I will do better next time:***\r\n- Finish reading HMM tutorial\r\n- Finish coding regarding Twitter API\r\n- Solve how to access listed companies' names via websites\r\n- Read KG paper if have time \r\n\r\n30/09/2024\r\n***Things I learned today:***\r\n- creating a proxy list requires downloading proxy software \r\n- the forward-backward algo: using a dynamic programming algo with message passing, can use to compute filtered/smoothed marginals. In a forward algo, we use filtered marginals using a predict-update cycle, the prediction uses one-step predict density. \r\nIn the update state, the observed data is absorbed using Bayes rule. In the forward-backward algo, we compute smoothed marginals using backward messages.\r\n- The Viterbi algo: compute the most probable sequence of hidden states, also has a forward-backward pass, however, we can use log domain here\r\n- The Baum-Welch algo: an Expectation-Maximisation algo for HMM.\r\n***Things I did well today:***\r\n- looking at creating a proxy list\r\n- checking code\r\n- reading HMM\r\n***Things I did not do well today:***\r\n- not reading KG paper\r\n- not finish proxy list yet\r\n***Things I will do better next time:***\r\n- finish proxy list\r\n- finish 2nd pass HMM\r\n- read KG paper if have time\r\n- solving coding regarding Twitter API
===================================================================
diff --git a/misc/Stock App Review.md b/misc/Stock App Review.md
--- a/misc/Stock App Review.md	
+++ b/misc/Stock App Review.md	
@@ -1854,4 +1854,23 @@
 - finish proxy list
 - finish 2nd pass HMM
 - read KG paper if have time
-- solving coding regarding Twitter API
\ No newline at end of file
+- solving coding regarding Twitter API
+
+1/10/2024
+***Things I learned today:***
+- HMM: a tool/model representing probability distributions over sequences of observations, for states. The states are not directly observable. Each state at each time depends on the previous state => first-order HMM.
+There are 5 elements characterizing a HMM: number of states in the model, number of distinct observations, state transition model, observation model, initial state distribution. 
+- To turn a companies list to a fully persistent data structure, we can transform it to a SQLite database
+
+***Things I did well today:***
+- finished coding companies list
+- 3rd pass HMM paper
+
+***Things I did not do well today:***
+- not reading KG paper
+- not finished proxy list
+
+***Things I will do better next time:***
+- Working on proxy list
+- Continuing 3rd pass HMM paper
+- continuing download texts code
